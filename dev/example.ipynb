{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "from config import *\n",
    "from sys import getsizeof\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import dateutil.parser as dp\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import datetime\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\Users\\\\MANDR\\\\Desktop\\\\1210\\\\gdepc\\\\static\\\\datasets\\\\beach-weather-stations-automated-sensors-1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Loading example dataset\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATASETS_ABS_PATHS\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBWSAS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m), inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# Replace spaces with underscores in column names\u001b[39;00m\n\u001b[0;32m      5\u001b[0m df\u001b[38;5;241m.\u001b[39mMeasurement_Timestamp \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df\u001b[38;5;241m.\u001b[39mMeasurement_Timestamp, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mI:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\MANDR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\MANDR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\MANDR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\MANDR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\MANDR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'c:\\\\Users\\\\MANDR\\\\Desktop\\\\1210\\\\gdepc\\\\static\\\\datasets\\\\beach-weather-stations-automated-sensors-1.csv'"
     ]
    }
   ],
   "source": [
    "# Loading example dataset\n",
    "\n",
    "df = pd.read_csv(DATASETS_ABS_PATHS[\"BWSAS\"])\n",
    "df.rename(columns=lambda x: x.replace(' ', '_'), inplace=True) # Replace spaces with underscores in column names\n",
    "df.Measurement_Timestamp = pd.to_datetime(df.Measurement_Timestamp, format=\"%m/%d/%Y %I:%M:%S %p\")\n",
    "df = df.convert_dtypes()\n",
    "df_dict = df.to_dict(orient=\"records\")\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station_Name                   | string\n",
      "Measurement_Timestamp          | datetime64[ns]\n",
      "Air_Temperature                | Float64\n",
      "Wet_Bulb_Temperature           | Float64\n",
      "Humidity                       | Int64\n",
      "Rain_Intensity                 | Float64\n",
      "Interval_Rain                  | Float64\n",
      "Total_Rain                     | Float64\n",
      "Precipitation_Type             | Int64\n",
      "Wind_Direction                 | Int64\n",
      "Wind_Speed                     | Float64\n",
      "Maximum_Wind_Speed             | Float64\n",
      "Barometric_Pressure            | Float64\n",
      "Solar_Radiation                | Int64\n",
      "Heading                        | Int64\n",
      "Battery_Life                   | Float64\n",
      "Measurement_Timestamp_Label    | string\n",
      "Measurement_ID                 | string\n"
     ]
    }
   ],
   "source": [
    "for n, t in df.dtypes.to_dict().items():\n",
    "    print(f\"{n:30} | {t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Station_Name</th>\n",
       "      <th>Measurement_Timestamp</th>\n",
       "      <th>Air_Temperature</th>\n",
       "      <th>Wet_Bulb_Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Rain_Intensity</th>\n",
       "      <th>Interval_Rain</th>\n",
       "      <th>Total_Rain</th>\n",
       "      <th>Precipitation_Type</th>\n",
       "      <th>Wind_Direction</th>\n",
       "      <th>Wind_Speed</th>\n",
       "      <th>Maximum_Wind_Speed</th>\n",
       "      <th>Barometric_Pressure</th>\n",
       "      <th>Solar_Radiation</th>\n",
       "      <th>Heading</th>\n",
       "      <th>Battery_Life</th>\n",
       "      <th>Measurement_Timestamp_Label</th>\n",
       "      <th>Measurement_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oak Street Weather Station</td>\n",
       "      <td>2015-05-22 15:00:00</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>7.0</td>\n",
       "      <td>55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2.8</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>780</td>\n",
       "      <td>322</td>\n",
       "      <td>12.0</td>\n",
       "      <td>05/22/2015 3:00 PM</td>\n",
       "      <td>OakStreetWeatherStation201505221500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oak Street Weather Station</td>\n",
       "      <td>2015-05-22 17:00:00</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>6.3</td>\n",
       "      <td>56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0</td>\n",
       "      <td>124</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>180</td>\n",
       "      <td>322</td>\n",
       "      <td>12.1</td>\n",
       "      <td>05/22/2015 5:00 PM</td>\n",
       "      <td>OakStreetWeatherStation201505221700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oak Street Weather Station</td>\n",
       "      <td>2015-05-22 18:00:00</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>6.5</td>\n",
       "      <td>54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0</td>\n",
       "      <td>156</td>\n",
       "      <td>1.9</td>\n",
       "      <td>3.4</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>127</td>\n",
       "      <td>322</td>\n",
       "      <td>12.1</td>\n",
       "      <td>05/22/2015 6:00 PM</td>\n",
       "      <td>OakStreetWeatherStation201505221800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oak Street Weather Station</td>\n",
       "      <td>2015-05-22 19:00:00</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>6.3</td>\n",
       "      <td>53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>1.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>67</td>\n",
       "      <td>322</td>\n",
       "      <td>12.1</td>\n",
       "      <td>05/22/2015 7:00 PM</td>\n",
       "      <td>OakStreetWeatherStation201505221900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oak Street Weather Station</td>\n",
       "      <td>2015-05-22 20:00:00</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>6.4</td>\n",
       "      <td>52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0</td>\n",
       "      <td>155</td>\n",
       "      <td>1.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>10</td>\n",
       "      <td>322</td>\n",
       "      <td>12.0</td>\n",
       "      <td>05/22/2015 8:00 PM</td>\n",
       "      <td>OakStreetWeatherStation201505222000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59139</th>\n",
       "      <td>Oak Street Weather Station</td>\n",
       "      <td>2017-11-08 00:00:00</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.8</td>\n",
       "      <td>60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>273</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1005.4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11.9</td>\n",
       "      <td>11/08/2017 12:00 AM</td>\n",
       "      <td>OakStreetWeatherStation201711082400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59140</th>\n",
       "      <td>Foster Weather Station</td>\n",
       "      <td>2017-11-08 00:00:00</td>\n",
       "      <td>2.89</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>58</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>278</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1004.4</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>15.2</td>\n",
       "      <td>11/08/2017 12:00 AM</td>\n",
       "      <td>FosterWeatherStation201711082400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59141</th>\n",
       "      <td>63rd Street Weather Station</td>\n",
       "      <td>2017-11-08 01:00:00</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.1</td>\n",
       "      <td>66</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.8</td>\n",
       "      <td>0</td>\n",
       "      <td>299</td>\n",
       "      <td>1.1</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1004.6</td>\n",
       "      <td>4</td>\n",
       "      <td>354</td>\n",
       "      <td>11.8</td>\n",
       "      <td>11/08/2017 1:00 AM</td>\n",
       "      <td>63rdStreetWeatherStation201711080100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59142</th>\n",
       "      <td>Oak Street Weather Station</td>\n",
       "      <td>2017-11-08 01:00:00</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>276</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1005.4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11/08/2017 1:00 AM</td>\n",
       "      <td>OakStreetWeatherStation201711080100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59143</th>\n",
       "      <td>Foster Weather Station</td>\n",
       "      <td>2017-11-08 01:00:00</td>\n",
       "      <td>2.28</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>60</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>273</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1004.1</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>15.2</td>\n",
       "      <td>11/08/2017 1:00 AM</td>\n",
       "      <td>FosterWeatherStation201711080100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59144 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Station_Name Measurement_Timestamp  Air_Temperature  \\\n",
       "0       Oak Street Weather Station   2015-05-22 15:00:00             <NA>   \n",
       "1       Oak Street Weather Station   2015-05-22 17:00:00             <NA>   \n",
       "2       Oak Street Weather Station   2015-05-22 18:00:00             <NA>   \n",
       "3       Oak Street Weather Station   2015-05-22 19:00:00             <NA>   \n",
       "4       Oak Street Weather Station   2015-05-22 20:00:00             <NA>   \n",
       "...                            ...                   ...              ...   \n",
       "59139   Oak Street Weather Station   2017-11-08 00:00:00              4.7   \n",
       "59140       Foster Weather Station   2017-11-08 00:00:00             2.89   \n",
       "59141  63rd Street Weather Station   2017-11-08 01:00:00              3.4   \n",
       "59142   Oak Street Weather Station   2017-11-08 01:00:00              4.2   \n",
       "59143       Foster Weather Station   2017-11-08 01:00:00             2.28   \n",
       "\n",
       "       Wet_Bulb_Temperature  Humidity  Rain_Intensity  Interval_Rain  \\\n",
       "0                       7.0        55             0.0            0.0   \n",
       "1                       6.3        56             0.0            0.0   \n",
       "2                       6.5        54             0.0            0.0   \n",
       "3                       6.3        53             0.0            0.0   \n",
       "4                       6.4        52             0.0            0.0   \n",
       "...                     ...       ...             ...            ...   \n",
       "59139                   1.8        60             0.0            0.0   \n",
       "59140                  <NA>        58            <NA>            0.0   \n",
       "59141                   1.1        66             0.0            0.0   \n",
       "59142                   1.4        59             0.0            0.0   \n",
       "59143                  <NA>        60            <NA>            0.0   \n",
       "\n",
       "       Total_Rain  Precipitation_Type  Wind_Direction  Wind_Speed  \\\n",
       "0             1.4                   0              63         1.9   \n",
       "1             1.4                   0             124         1.5   \n",
       "2             1.4                   0             156         1.9   \n",
       "3             1.4                   0             150         1.4   \n",
       "4             1.4                   0             155         1.1   \n",
       "...           ...                 ...             ...         ...   \n",
       "59139         0.0                   0             273         0.8   \n",
       "59140        <NA>                <NA>             278         2.1   \n",
       "59141        28.8                   0             299         1.1   \n",
       "59142         0.0                   0             276         0.8   \n",
       "59143        <NA>                <NA>             273         1.6   \n",
       "\n",
       "       Maximum_Wind_Speed  Barometric_Pressure  Solar_Radiation  Heading  \\\n",
       "0                     2.8                 <NA>              780      322   \n",
       "1                     2.3                 <NA>              180      322   \n",
       "2                     3.4                 <NA>              127      322   \n",
       "3                     4.5                 <NA>               67      322   \n",
       "4                     2.3                 <NA>               10      322   \n",
       "...                   ...                  ...              ...      ...   \n",
       "59139                 1.5               1005.4                0        2   \n",
       "59140                 2.3               1004.4                0     <NA>   \n",
       "59141                 2.2               1004.6                4      354   \n",
       "59142                 1.7               1005.4                1        2   \n",
       "59143                 1.7               1004.1                0     <NA>   \n",
       "\n",
       "       Battery_Life Measurement_Timestamp_Label  \\\n",
       "0              12.0          05/22/2015 3:00 PM   \n",
       "1              12.1          05/22/2015 5:00 PM   \n",
       "2              12.1          05/22/2015 6:00 PM   \n",
       "3              12.1          05/22/2015 7:00 PM   \n",
       "4              12.0          05/22/2015 8:00 PM   \n",
       "...             ...                         ...   \n",
       "59139          11.9         11/08/2017 12:00 AM   \n",
       "59140          15.2         11/08/2017 12:00 AM   \n",
       "59141          11.8          11/08/2017 1:00 AM   \n",
       "59142          12.0          11/08/2017 1:00 AM   \n",
       "59143          15.2          11/08/2017 1:00 AM   \n",
       "\n",
       "                             Measurement_ID  \n",
       "0       OakStreetWeatherStation201505221500  \n",
       "1       OakStreetWeatherStation201505221700  \n",
       "2       OakStreetWeatherStation201505221800  \n",
       "3       OakStreetWeatherStation201505221900  \n",
       "4       OakStreetWeatherStation201505222000  \n",
       "...                                     ...  \n",
       "59139   OakStreetWeatherStation201711082400  \n",
       "59140      FosterWeatherStation201711082400  \n",
       "59141  63rdStreetWeatherStation201711080100  \n",
       "59142   OakStreetWeatherStation201711080100  \n",
       "59143      FosterWeatherStation201711080100  \n",
       "\n",
       "[59144 rows x 18 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# proto files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating proto template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_proto_template(package_name: str, df: pd.DataFrame):\n",
    "    d = []\n",
    "    for name, dtype in df.dtypes.to_dict().items():\n",
    "        match dtype:\n",
    "            case \"string\":\n",
    "                d.append([\"string\", name])\n",
    "            case \"Float64\":\n",
    "                d.append([\"float\", name])\n",
    "            case \"Int64\":\n",
    "                d.append([\"int64\", name])\n",
    "            case \"datetime64[ns]\":\n",
    "                d.append([\"google.protobuf.Timestamp\", name])\n",
    "    \n",
    "    template = f'\\\n",
    "syntax = \"proto3\";\\n\\\n",
    "import \"google/protobuf/timestamp.proto\";\\n\\n\\\n",
    "package {package_name};\\n\\\n",
    "message {package_name}_message {{\\n\\\n",
    "'\n",
    "\n",
    "    template += \"\\n\".join(f\"    {e[0]} {e[1]} = {d.index(e) + 1};\" for e in d)\n",
    "    template += \"\\n}\"\n",
    "    return template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does NOT work (yoinked from net)\n",
    "\n",
    "import pandas as pd\n",
    "from google.protobuf.descriptor_pb2 import FieldDescriptorProto\n",
    "from google.protobuf.descriptor_pb2 import FileDescriptorProto\n",
    "from google.protobuf.compiler.plugin_pb2 import CodeGeneratorRequest\n",
    "from google.protobuf.compiler.plugin_pb2 import CodeGeneratorResponse\n",
    "\n",
    "def create_protobuf_template(df):\n",
    "    # Create a FileDescriptorProto\n",
    "    file_descriptor_proto = FileDescriptorProto()\n",
    "\n",
    "    # Create a message type for the DataFrame\n",
    "    message_type = file_descriptor_proto.message_type.add()\n",
    "    message_type.name = \"DataFrame\"\n",
    "\n",
    "    # Iterate over the columns of the DataFrame\n",
    "    for column_name, column_type in df.dtypes.items():\n",
    "        # Create a field descriptor for each column\n",
    "        field_descriptor = message_type.field.add()\n",
    "        field_descriptor.name = column_name\n",
    "\n",
    "        # Map Pandas dtype to Protobuf field type\n",
    "        if column_type == \"int64\":\n",
    "            field_descriptor.type = FieldDescriptorProto.TYPE_INT64\n",
    "        elif column_type == \"float64\":\n",
    "            field_descriptor.type = FieldDescriptorProto.TYPE_DOUBLE\n",
    "        elif column_type == \"bool\":\n",
    "            field_descriptor.type = FieldDescriptorProto.TYPE_BOOL\n",
    "        elif column_type == \"object\":\n",
    "            field_descriptor.type = FieldDescriptorProto.TYPE_STRING\n",
    "        elif column_type == \"datetime64[ns]\":\n",
    "            field_descriptor.type = FieldDescriptorProto.TYPE_STRING\n",
    "        else:\n",
    "            field_descriptor.type = FieldDescriptorProto.TYPE_STRING\n",
    "\n",
    "    # Return the serialized FileDescriptorProto\n",
    "    return file_descriptor_proto.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "syntax = \"proto3\";\n",
      "import \"google/protobuf/timestamp.proto\";\n",
      "\n",
      "package m_beach;\n",
      "message m_beach_message {\n",
      "    string Station_Name = 1;\n",
      "    google.protobuf.Timestamp Measurement_Timestamp = 2;\n",
      "    float Air_Temperature = 3;\n",
      "    float Wet_Bulb_Temperature = 4;\n",
      "    int64 Humidity = 5;\n",
      "    float Rain_Intensity = 6;\n",
      "    float Interval_Rain = 7;\n",
      "    float Total_Rain = 8;\n",
      "    int64 Precipitation_Type = 9;\n",
      "    int64 Wind_Direction = 10;\n",
      "    float Wind_Speed = 11;\n",
      "    float Maximum_Wind_Speed = 12;\n",
      "    float Barometric_Pressure = 13;\n",
      "    int64 Solar_Radiation = 14;\n",
      "    int64 Heading = 15;\n",
      "    float Battery_Life = 16;\n",
      "    string Measurement_Timestamp_Label = 17;\n",
      "    string Measurement_ID = 18;\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# proto template exaple\n",
    "package_name = \"m_beach\"\n",
    "\n",
    "protobuf_template = create_proto_template(package_name, df)\n",
    "print(protobuf_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the byte string to a file.\n",
    "proto_filename = package_name + \".proto\"\n",
    "\n",
    "with open(path.join(PROTOS_FOLDER_ABS_PATH, proto_filename), \"w\") as f:\n",
    "    f.write(protobuf_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compiling proto file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proto file compilation funciton (creates {proto_filename}_pb2.py file)\n",
    "def compile_proto(proto_filename):\n",
    "    proto_abs_path = path.join(PROTOS_FOLDER, proto_filename)\n",
    "    compile_proto_command = f\"{PROTOC_EXE} --python_out=. {proto_abs_path} \"\n",
    "\n",
    "    os.system(compile_proto_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proto file compilation exapmle\n",
    "\n",
    "compile_proto(proto_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading data into python descriptor of proto message "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util function for timestamps calculaitons, conversion and validation\n",
    "def str2unix(dt: str) -> tuple[int, int]:\n",
    "    # Converts ISO8601 timstamp string into POSIX timestamp tuple (seconds, nanoseconds)\n",
    "    dt_unix = dp.parse(dt).timestamp()\n",
    "    \n",
    "    seconds = int(dt_unix)\n",
    "    nanos   = int(dt_unix % 1 * 1e9)\n",
    "\n",
    "    return (seconds, nanos)\n",
    "\n",
    "\n",
    "def pdTimestamp2unix(dt: pd.Timestamp) -> tuple[int, int]:\n",
    "    return str2unix(dt.isoformat())\n",
    "\n",
    "\n",
    "def datetime_valid(dt_str):\n",
    "    try:\n",
    "        datetime.datetime.fromisoformat(dt_str)\n",
    "    except:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# json encoder and decoder for timestamp conversion handling\n",
    "class _JSONDecoder(json.JSONDecoder):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        json.JSONDecoder.__init__(\n",
    "            self, object_hook=self.object_hook, *args, **kwargs)\n",
    "\n",
    "    def object_hook(self, obj):\n",
    "        ret = {}\n",
    "        for key, value in obj.items():\n",
    "            if key in {'timestamp', 'whatever'}:\n",
    "                ret[key] = datetime.fromisoformat(value) \n",
    "            else:\n",
    "                ret[key] = value\n",
    "        return ret\n",
    "\n",
    "    \n",
    "class _JSONEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (datetime.date, datetime.datetime, pd.Timestamp, )):\n",
    "            return obj.isoformat()\n",
    "        return json.JSONEncoder.default(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fill package_message with data from DataFrame\n",
    "def fill_package_from_df(package_name: str, df_dict: list[dict], item: int):\n",
    "    # df_dict - df.to_dict(orient=\"records\"); item - number of record in df\n",
    "    package = importlib.import_module(PROTOS_FOLDER + \".\" + package_name + \"_pb2\")\n",
    "    package_message = getattr(package, package_name + \"_message\")()\n",
    "\n",
    "    for field, value in df_dict[item].items():\n",
    "        if value is None:\n",
    "            continue\n",
    "        if type(value) is pd.Timestamp:\n",
    "            seconds, nanos = pdTimestamp2unix(value)\n",
    "            \n",
    "            setattr(getattr(package_message, field), \"seconds\", seconds)\n",
    "            setattr(getattr(package_message, field), \"nanos\", nanos)\n",
    "        else:\n",
    "            try:\n",
    "                setattr(package_message, field, value)\n",
    "            except Exception as e:\n",
    "                print(f\"Exception: {e} | field: {field} | value: {value}\")\n",
    "    \n",
    "    return package_message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fill package_message with data from dict\n",
    "def fill_package_from_dict(package_name: str, message: dict):\n",
    "    # message - dict (json)\n",
    "    package = importlib.import_module(PROTOS_FOLDER + \".\" + package_name + \"_pb2\")\n",
    "    package_message = getattr(package, package_name + \"_message\")()\n",
    "\n",
    "    for field, value in message.items():\n",
    "        if value is None:\n",
    "            continue\n",
    "        if datetime_valid(value):\n",
    "            seconds, nanos = str2unix(value)\n",
    "            setattr(getattr(package_message, field), \"seconds\", seconds)\n",
    "            setattr(getattr(package_message, field), \"nanos\", nanos)\n",
    "        elif type(value) is pd.Timestamp:\n",
    "            seconds, nanos = pdTimestamp2unix(value)\n",
    "            setattr(getattr(package_message, field), \"seconds\", seconds)\n",
    "            setattr(getattr(package_message, field), \"nanos\", nanos)\n",
    "        else:\n",
    "            try:\n",
    "                setattr(package_message, field, value)\n",
    "            except Exception as e:\n",
    "                print(f\"Exception: {e} | field: {field} | value: {value}\")\n",
    "    \n",
    "    return package_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# example of filling message with data from df and dicts with pd.Timestamp and timestamp in str \n",
    "# \n",
    "# NOTE: json format does not specify timestamp format, so i used most commonly used - ISO8601, \n",
    "# using custom JSONEncoder/JSONDecoder\n",
    "\n",
    "item = 0\n",
    "message = df_dict[item] # dict with pd.Timestamp as timestamp\n",
    "message_json = json.dumps(message, cls=_JSONEncoder)\n",
    "\n",
    "package_message = fill_package_from_df(package_name, df_dict, item)\n",
    "package_message1 = fill_package_from_dict(package_name, message)\n",
    "# message = json.loads(message_json) # dict with str ISO8601 as timestamp\n",
    "package_message2 = fill_package_from_dict(package_name, json.loads(message_json))\n",
    "\n",
    "print(package_message == package_message1 == package_message2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Station_Name': 'Oak Street Weather Station',\n",
       "  'Measurement_Timestamp': Timestamp('2015-05-22 15:00:00'),\n",
       "  'Air_Temperature': None,\n",
       "  'Wet_Bulb_Temperature': 7.0,\n",
       "  'Humidity': 55,\n",
       "  'Rain_Intensity': 0.0,\n",
       "  'Interval_Rain': 0.0,\n",
       "  'Total_Rain': 1.4,\n",
       "  'Precipitation_Type': 0,\n",
       "  'Wind_Direction': 63,\n",
       "  'Wind_Speed': 1.9,\n",
       "  'Maximum_Wind_Speed': 2.8,\n",
       "  'Barometric_Pressure': None,\n",
       "  'Solar_Radiation': 780,\n",
       "  'Heading': 322,\n",
       "  'Battery_Life': 12.0,\n",
       "  'Measurement_Timestamp_Label': '05/22/2015 3:00 PM',\n",
       "  'Measurement_ID': 'OakStreetWeatherStation201505221500'},\n",
       " '{\"Station_Name\": \"Oak Street Weather Station\", \"Measurement_Timestamp\": \"2015-05-22T15:00:00\", \"Air_Temperature\": null, \"Wet_Bulb_Temperature\": 7.0, \"Humidity\": 55, \"Rain_Intensity\": 0.0, \"Interval_Rain\": 0.0, \"Total_Rain\": 1.4, \"Precipitation_Type\": 0, \"Wind_Direction\": 63, \"Wind_Speed\": 1.9, \"Maximum_Wind_Speed\": 2.8, \"Barometric_Pressure\": null, \"Solar_Radiation\": 780, \"Heading\": 322, \"Battery_Life\": 12.0, \"Measurement_Timestamp_Label\": \"05/22/2015 3:00 PM\", \"Measurement_ID\": \"OakStreetWeatherStation201505221500\"}')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message, message_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## serializing/deserializing + io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize/deserialize functions\n",
    "\n",
    "def proto_serialize(package_message):\n",
    "    return package_message.SerializeToString()\n",
    "\n",
    "\n",
    "def proto_deserialize(package_message_serialized, package_name: str):\n",
    "    package = importlib.import_module(PROTOS_FOLDER + \".\" + package_name + \"_pb2\")\n",
    "    package_class = getattr(package, package_name + \"_message\")\n",
    "\n",
    "    package_message = package_class.FromString(package_message_serialized)\n",
    "    \n",
    "    return package_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IO funcitons for package_message \n",
    "\n",
    "# writing package_message to the file\n",
    "def write_package(package_message, package_bin_filename):\n",
    "    with open(package_bin_filename, \"wb\") as f:\n",
    "        package_message_serialized = proto_serialize(package_message)\n",
    "        f.write(package_message_serialized)\n",
    "    \n",
    "    return package_message_serialized\n",
    "    \n",
    "\n",
    "\n",
    "# reading package_message from the file\n",
    "def read_package(package_bin_filename, package_name):\n",
    "    with open(package_bin_filename, \"rb\") as f:\n",
    "        package_message_serialized = f.read()\n",
    "        \n",
    "        package_message = proto_deserialize(package_message_serialized, package_name)\n",
    "\n",
    "    return package_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example usage of serialization\n",
    "package_bin_filename = package_name + \".bin\"\n",
    "\n",
    "package_message_serialized   = write_package(package_message, package_bin_filename)\n",
    "package_message_deserialized = read_package(package_bin_filename, package_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dumping json file\n",
    "package_json_filename = package_name + \".json\"\n",
    "# with open(package_name + \".json\", \"w\") as f:\n",
    "#     json.dump(message, f, cls=_JSONEncoder)\n",
    "# with open(package_name + \"1.json\", \"w\") as f:\n",
    "#     f.write(message_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util funcitons for bit <-> byte conversion and size finding\n",
    "\n",
    "def to_bits(byte_string: bytes) -> str:\n",
    "    # Convert byte string into bit string\n",
    "    return bin(int.from_bytes(byte_string, byteorder='big'))[2:]  # Remove '0b' prefix from binary string\n",
    "\n",
    "\n",
    "def to_bytes(bit_string: str) -> bytes:\n",
    "    # Convert bit string into byte string\n",
    "    return int(bit_string, 2).to_bytes((len(bit_string) + 7) // 8, byteorder='big')\n",
    "\n",
    "\n",
    "def bitsize(data: bytes | str) -> int:\n",
    "    if type(data) == bytes:\n",
    "        return len(to_bits(data))\n",
    "    elif type(data) == str and set(data) <= {'0', '1'}:\n",
    "        return len(data)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('101000011010010011110110000101101011001000000101001101110100011100100110010101100101011101000010000001010111011001010110000101110100011010000110010101110010001000000101001101110100011000010111010001101001011011110110111000010010000001100000100011000000101101001111110010101010000001010010010100000000000000001110000001000000001010000011011101000101001100110011001110110011001111110101000000111111010111010011001100110011111100110011111101100101001100110011001100110011010000000111000010001100000001100111100011000010000000101000010100000001000000000000000001000000010000011000101000000001000100100011000000110101001011110011001000110010001011110011001000110000001100010011010100100000001100110011101000110000001100000010000001010000010011011001001000000001001000110100111101100001011010110101001101110100011100100110010101100101011101000101011101100101011000010111010001101000011001010111001001010011011101000110000101110100011010010110111101101110001100100011000000110001001101010011000000110101001100100011001000110001001101010011000000110000',\n",
       " 1044,\n",
       " True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of using bit/byte conversion + size finding\n",
    "to_bits(package_message_serialized), bitsize(package_message_serialized), to_bytes(to_bits(package_message_serialized)) == package_message_serialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message size (dict):                464\n",
      "message_json size (str of json):    563\n",
      "\n",
      "bin file size:                      131\n",
      "json file size:                     522\n",
      "\n",
      "package_message size:               80\n",
      "package_message_serialized bits:    1044\n",
      "\n",
      "ratio of serialized/json files:     3.984732824427481\n",
      "package_message == package_message_deserialized: True\n"
     ]
    }
   ],
   "source": [
    "# sizes comparison\n",
    "print(f\"message size (dict):                {getsizeof(message)}\")\n",
    "print(f\"message_json size (str of json):    {getsizeof(message_json)}\")\n",
    "print()\n",
    "print(f\"bin file size:                      {os.path.getsize(package_bin_filename)}\")\n",
    "print(f\"json file size:                     {os.path.getsize(package_json_filename)}\")\n",
    "print()\n",
    "print(f\"package_message size:               {getsizeof(package_message)}\")\n",
    "print(f\"package_message_serialized bits:    {bitsize(package_message_serialized)}\")\n",
    "print()\n",
    "print(f\"ratio of serialized/json files:     {os.path.getsize(package_json_filename) / os.path.getsize(package_bin_filename)}\")\n",
    "print(f\"package_message == package_message_deserialized: {package_message == package_message_deserialized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Station_Name', 'Measurement_Timestamp', 'Air_Temperature', 'Wet_Bulb_Temperature', 'Humidity', 'Rain_Intensity', 'Interval_Rain', 'Total_Rain', 'Precipitation_Type', 'Wind_Direction', 'Wind_Speed', 'Maximum_Wind_Speed', 'Barometric_Pressure', 'Solar_Radiation', 'Heading', 'Battery_Life', 'Measurement_Timestamp_Label', 'Measurement_ID']\n"
     ]
    }
   ],
   "source": [
    "print([field.name for field in package_message.DESCRIPTOR.fields])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## libraries algorithms comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import zlib, gzip, bz2, lzma, lz4.frame, zstd, brotli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pb_ratio_zlib:        1.079627714581179      \n",
      "pb_ratio_gzip:        0.9839773798303487      \n",
      "pb_ratio_bz2:         0.7462473195139385      \n",
      "pb_ratio_lzma:        0.7092391304347826      \n",
      "pb_ration_lz4:        0.9038961038961039      \n",
      "pb_ration_zstd(1):    0.9338103756708408      \n",
      "pb_ration_brotli(11): 0.999043062200957\n"
     ]
    }
   ],
   "source": [
    "pb_msg = package_message_serialized\n",
    "\n",
    "pb_ratio_zlib     = bitsize(pb_msg) / bitsize(zlib.compress(pb_msg))\n",
    "pb_ratio_gzip     = bitsize(pb_msg) / bitsize(gzip.compress(pb_msg))\n",
    "pb_ratio_bz2      = bitsize(pb_msg) / bitsize(bz2.compress(pb_msg))\n",
    "pb_ratio_lzma     = bitsize(pb_msg) / bitsize(lzma.compress(pb_msg))\n",
    "pb_ratio_lz4      = bitsize(pb_msg) / bitsize(lz4.frame.compress(pb_msg))\n",
    "pb_ratio_zstd     = bitsize(pb_msg) / bitsize(zstd.compress(pb_msg, 1))\n",
    "pb_ratio_brotli   = bitsize(pb_msg) / bitsize(brotli.compress(pb_msg))\n",
    "\n",
    "\n",
    "\n",
    "print(f'pb_ratio_zlib:        {pb_ratio_zlib}\\\n",
    "      \\npb_ratio_gzip:        {pb_ratio_gzip}\\\n",
    "      \\npb_ratio_bz2:         {pb_ratio_bz2}\\\n",
    "      \\npb_ratio_lzma:        {pb_ratio_lzma}\\\n",
    "      \\npb_ration_lz4:        {pb_ratio_lz4}\\\n",
    "      \\npb_ration_zstd(1):    {pb_ratio_zstd}\\\n",
    "      \\npb_ration_brotli(11): {pb_ratio_brotli}\\\n",
    "'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j_ratio_zlib:         1.7061708214139764      \n",
      "j_ratio_gzip:         1.6430539157811885      \n",
      "j_ratio_bz2:          1.5220561429092234      \n",
      "j_ratio_lzma:         1.3733552631578947      \n",
      "j_ration_lz4:         1.1986792994544933      \n",
      "j_ration_zstd(1):     1.5451517394522576      \n",
      "j_ration_brotli(11):  1.928406466512702\n"
     ]
    }
   ],
   "source": [
    "j_msg = bytes(message_json, \"utf-8\")\n",
    "\n",
    "j_ratio_zlib      = bitsize(j_msg)  / bitsize(zlib.compress(j_msg))\n",
    "j_ratio_gzip      = bitsize(j_msg)  / bitsize(gzip.compress(j_msg))\n",
    "j_ratio_bz2       = bitsize(j_msg)  / bitsize(bz2.compress(j_msg)) \n",
    "j_ratio_lzma      = bitsize(j_msg)  / bitsize(lzma.compress(j_msg))\n",
    "j_ratio_lz4       = bitsize(j_msg)  / bitsize(lz4.frame.compress(j_msg))\n",
    "j_ratio_zstd      = bitsize(j_msg)  / bitsize(zstd.compress(j_msg, 1))\n",
    "j_ratio_brotli    = bitsize(j_msg)  / bitsize(brotli.compress(j_msg))\n",
    "\n",
    "print(f'j_ratio_zlib:         {j_ratio_zlib}\\\n",
    "      \\nj_ratio_gzip:         {j_ratio_gzip}\\\n",
    "      \\nj_ratio_bz2:          {j_ratio_bz2}\\\n",
    "      \\nj_ratio_lzma:         {j_ratio_lzma}\\\n",
    "      \\nj_ration_lz4:         {j_ratio_lz4}\\\n",
    "      \\nj_ration_zstd(1):     {j_ratio_zstd}\\\n",
    "      \\nj_ration_brotli(11):  {j_ratio_brotli}\\\n",
    "'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huffman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huffman encoding algorithm\n",
    "\n",
    "import heapq\n",
    "from collections import defaultdict\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, char=None, freq=0, left=None, right=None):\n",
    "        self.char = char\n",
    "        self.freq = freq\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        if self.freq == other.freq:\n",
    "            return self.char < other.char if self.char and other.char else False\n",
    "        return self.freq < other.freq\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.freq == other.freq and self.char == other.char\n",
    "\n",
    "def build_huffman_tree(freq_dict):\n",
    "    priority_queue = [Node(char, freq) for char, freq in freq_dict.items()]\n",
    "    heapq.heapify(priority_queue)\n",
    "\n",
    "    while len(priority_queue) > 1:\n",
    "        left = heapq.heappop(priority_queue)\n",
    "        right = heapq.heappop(priority_queue)\n",
    "        new_node = Node(freq=left.freq + right.freq, left=left, right=right)\n",
    "        heapq.heappush(priority_queue, new_node)\n",
    "\n",
    "    return priority_queue[0]\n",
    "\n",
    "\n",
    "def build_frequency_dict(data):\n",
    "    freq_dict = defaultdict(int)\n",
    "    for char in data:\n",
    "        freq_dict[char] += 1\n",
    "    return freq_dict\n",
    "\n",
    "def build_codewords(node, current_code=\"\", code_dict=None):\n",
    "    if code_dict is None:\n",
    "        code_dict = {}\n",
    "\n",
    "    if node.char is not None:\n",
    "        code_dict[node.char] = current_code\n",
    "        return code_dict\n",
    "\n",
    "    code_dict = build_codewords(node.left, current_code + \"0\", code_dict)\n",
    "    code_dict = build_codewords(node.right, current_code + \"1\", code_dict)\n",
    "\n",
    "    return code_dict\n",
    "\n",
    "def huffman_encode(data):\n",
    "    freq_dict = build_frequency_dict(data)\n",
    "    huffman_tree = build_huffman_tree(freq_dict)\n",
    "    codewords = build_codewords(huffman_tree)\n",
    "\n",
    "    encoded_data = \"\".join(codewords[char] for char in data)\n",
    "    return encoded_data, huffman_tree\n",
    "\n",
    "def huffman_decode(encoded_data, huffman_tree):\n",
    "    decoded_data = bytearray()\n",
    "    current_node = huffman_tree\n",
    "\n",
    "    for bit in encoded_data:\n",
    "        if bit == \"0\":\n",
    "            current_node = current_node.left\n",
    "        else:\n",
    "            current_node = current_node.right\n",
    "\n",
    "        if current_node.char is not None:\n",
    "            decoded_data.append(ord(current_node.char))\n",
    "            current_node = huffman_tree\n",
    "\n",
    "    return bytes(decoded_data)\n",
    "\n",
    "\n",
    "def serialize_tree(node):\n",
    "    if node.char is not None:\n",
    "        return '1' + bin(node.char)[2:].zfill(8)\n",
    "    else:\n",
    "        return '0' + serialize_tree(node.left) + serialize_tree(node.right)\n",
    "\n",
    "\n",
    "def deserialize_tree(data):\n",
    "    def helper(index):\n",
    "        if data[index] == '1':\n",
    "            char = chr(int(data[index + 1:index + 9], 2))\n",
    "            return Node(char=char), index + 9\n",
    "        else:\n",
    "            left, index = helper(index + 1)\n",
    "            right, index = helper(index)\n",
    "            return Node(left=left, right=right), index\n",
    "\n",
    "    root, _ = helper(0)\n",
    "    return root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data:      b'\\n\\x1aOak Street Weather Station\\x12\\x06\\x08\\xc0\\xb4\\xfc\\xaa\\x05%\\x00\\x00\\xe0@(7E33\\xb3?P?]33\\xf3?e333@p\\x8c\\x06x\\xc2\\x02\\x85\\x01\\x00\\x00@A\\x8a\\x01\\x1205/22/2015 3:00 PM\\x92\\x01#OakStreetWeatherStation201505221500'\n",
      "Decoded data:       b'\\n\\x1aOak Street Weather Station\\x12\\x06\\x08\\xc0\\xb4\\xfc\\xaa\\x05%\\x00\\x00\\xe0@(7E33\\xb3?P?]33\\xf3?e333@p\\x8c\\x06x\\xc2\\x02\\x85\\x01\\x00\\x00@A\\x8a\\x01\\x1205/22/2015 3:00 PM\\x92\\x01#OakStreetWeatherStation201505221500'\n",
      "Decoded data == Original data: True\n",
      "Encoded data:       10011011011010001011000101111011010100001110100011100110011101101001101011000001111001110011001000111010100001110000111100110111001011011001010111010101001100010001101000101011111011111010100011001000010010100110100110010010010011011100011001100110011011111111111111110011111101000010011001110111101111111100001100110011001000111010011000010101001110111010010101000010011101111010100101001001000110010100111111110110101101011101100101000000000001010000001011111101101111010001110111010101010111010111100010000001100011111011011011001011000101111010000111010001110011001110011010110000011110011100110010001100001110000111100110111001011011000000010111111011011010111011000000001111101101101010101\n",
      "Serialized tree:    00001001100101011000010010100000001001011111010011111001100110000010100110110101110101101101001110000001000000001001100000000110001100110010010010100000110100010101010101111011010010010110100001011100001011110000101101011011010101011011001100001010100111011100100001001001011001010001011011110010000100010000101001100001011100010100000100000010100000101011100001011110000001000001101000100100010110111001000110101001000110010011011110011101001111100111111111000010110010101001000001001101010101110100001010100001000000010100110001100111111\n",
      "\n",
      "Encoded data bit length:    695\n",
      "Serialized tree bit length: 539\n",
      "Compression ratio:          0.8460291734197731\n"
     ]
    }
   ],
   "source": [
    "# example of huffman usage\n",
    "\n",
    "encoded_data, huffman_tree = huffman_encode(package_message_serialized)\n",
    "\n",
    "# Serialize the Huffman tree\n",
    "serialized_tree = serialize_tree(huffman_tree)\n",
    "\n",
    "# Deserialize the Huffman tree\n",
    "deserialized_tree = deserialize_tree(serialized_tree)\n",
    "\n",
    "# The rest of the code remains the same...\n",
    "# encoded_data = \"\".join(build_codewords(deserialized_tree)[char] for char in package_message_serialized)\n",
    "decoded_data = huffman_decode(encoded_data, deserialized_tree)\n",
    "\n",
    "print(f\"Original data:      {package_message_serialized}\")\n",
    "print(f\"Decoded data:       {decoded_data}\")\n",
    "print(f\"Decoded data == Original data: {package_message_serialized == decoded_data}\")\n",
    "print(f\"Encoded data:       {encoded_data}\")\n",
    "print(f\"Serialized tree:    {serialized_tree}\")\n",
    "print()\n",
    "print(f\"Encoded data bit length:    {bitsize(encoded_data)}\")\n",
    "print(f\"Serialized tree bit length: {bitsize(serialized_tree)}\")\n",
    "print(f\"Compression ratio:          {(bitsize(encoded_data) + bitsize(serialized_tree)) / bitsize(package_message_serialized)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rANS PyComP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counter = Counter(pb_msg)\n",
    "\n",
    "pb_msg_chr = ''.join([chr(i) for i in counter.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.PyComP import ANS\n",
    "msg_counter = Counter(pb_msg_chr)\n",
    "ans = ANS.rANS(list(msg_counter.keys()), list(msg_counter.values()))\n",
    "msg_enc, final_state = ans.encode([chr(i) for i in pb_msg], 0)\n",
    "msg_dec = ''.join(ans.decode(msg_enc, final_state))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\\n\\x1aOak StreWhion\\x12\\x06\\x08À´üª\\x05%\\x00à@(7E3³?P]óp\\x8cxÂ\\x02\\x85\\x01A\\x8a05/21:M\\x92#',\n",
       " '\\n\\x1aOak StreWhion\\x12\\x06\\x08À´üª\\x05%\\x00à@(7E3³?P]óp\\x8cxÂ\\x02\\x85\\x01A\\x8a05/21:M\\x92#',\n",
       " '10101100111110011010101001110001101010001010010100010101000101110111101000011001001100101110001100010110001011011111100110000101111010001101101000110101010001111000100011101110010010111101100111111101111011010000100110001001111101111010110100001111000110010010101100010111100000111000100011000001001111110000001110001101001100101101111110110110101110111100101111100000000111010101010111101011010101011011011001011000100110010110011100111010000101101010000100001000011000110111000100010111101001011110000101000010011011110010111011000110111111111111001111110001100011000101100100001100110100111110111010011111101100101101101111010011011111110001011001101110111011101101000010011110100100110011010100111100100011110001110011101011000000100111011',\n",
       " 743)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pb_msg_chr, pb_msg_chr, msg_enc[2:], len(msg_enc[2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cooking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_paths = DATASETS_ABS_PATHS.values()\n",
    "# dfs = \n",
    "# df = pd.read_csv(DATASETS_ABS_PATHS[\"BWSAS\"])\n",
    "# df.rename(columns=lambda x: x.replace(' ', '_'), inplace=True) # Replace spaces with underscores in column names\n",
    "# df.Measurement_Timestamp = pd.to_datetime(df.Measurement_Timestamp, format=\"%m/%d/%Y %I:%M:%S %p\")\n",
    "# df = df.convert_dtypes()\n",
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ASCCDCV': 'static\\\\datasets\\\\all_sites_combined_concentration_data_clean_version.xlsx',\n",
       " 'ARGAZAL': 'static\\\\datasets\\\\argentine_all_zones_all_winds.xlsx',\n",
       " 'ARMAZAL': 'static\\\\datasets\\\\armourdale_all_zones_all_winds.xlsx',\n",
       " 'BWQAS': 'static\\\\datasets\\\\beach_water_quality_automated_sensors_1.csv',\n",
       " 'BWSAS': 'static\\\\datasets\\\\beach_weather_stations_automated_sensors_1.csv',\n",
       " 'DTFSHZPG': 'static\\\\datasets\\\\data_table_for_science_hub_zhou_paper_gullett.xlsx',\n",
       " 'IOTNL': 'static\\\\datasets\\\\iot_network_logs.csv',\n",
       " 'IOTTEMP': 'static\\\\datasets\\\\iot_temp.csv',\n",
       " 'IOT1': 'static\\\\datasets\\\\iotpond1.csv',\n",
       " 'IOT2': 'static\\\\datasets\\\\iotpond2.csv',\n",
       " 'IOT3': 'static\\\\datasets\\\\iotpond3.csv',\n",
       " 'IOT4': 'static\\\\datasets\\\\iotpond4.csv',\n",
       " 'IOT6': 'static\\\\datasets\\\\iotpond6.csv',\n",
       " 'IOT7': 'static\\\\datasets\\\\iotpond7.csv',\n",
       " 'IOT8': 'static\\\\datasets\\\\iotpond8.csv',\n",
       " 'IOT9': 'static\\\\datasets\\\\iotpond9.csv',\n",
       " 'IOT10': 'static\\\\datasets\\\\iotpond10.csv',\n",
       " 'IOT11': 'static\\\\datasets\\\\iotpond11.csv',\n",
       " 'IOT12': 'static\\\\datasets\\\\iotpond12.csv',\n",
       " 'TAZAW': 'static\\\\datasets\\\\turner_all_zones_all_winds.xls'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASETS_REL_PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paths = list(DATASETS_REL_PATHS.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['static\\\\datasets\\\\all_sites_combined_concentration_data_clean_version.xlsx',\n",
       " 'static\\\\datasets\\\\argentine_all_zones_all_winds.xlsx',\n",
       " 'static\\\\datasets\\\\armourdale_all_zones_all_winds.xlsx',\n",
       " 'static\\\\datasets\\\\beach_water_quality_automated_sensors_1.csv',\n",
       " 'static\\\\datasets\\\\beach_weather_stations_automated_sensors_1.csv',\n",
       " 'static\\\\datasets\\\\data_table_for_science_hub_zhou_paper_gullett.xlsx',\n",
       " 'static\\\\datasets\\\\iot_network_logs.csv',\n",
       " 'static\\\\datasets\\\\iot_temp.csv',\n",
       " 'static\\\\datasets\\\\iotpond1.csv',\n",
       " 'static\\\\datasets\\\\iotpond2.csv',\n",
       " 'static\\\\datasets\\\\iotpond3.csv',\n",
       " 'static\\\\datasets\\\\iotpond4.csv',\n",
       " 'static\\\\datasets\\\\iotpond6.csv',\n",
       " 'static\\\\datasets\\\\iotpond7.csv',\n",
       " 'static\\\\datasets\\\\iotpond8.csv',\n",
       " 'static\\\\datasets\\\\iotpond9.csv',\n",
       " 'static\\\\datasets\\\\iotpond10.csv',\n",
       " 'static\\\\datasets\\\\iotpond11.csv',\n",
       " 'static\\\\datasets\\\\iotpond12.csv',\n",
       " 'static\\\\datasets\\\\turner_all_zones_all_winds.xls']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>room_id/id</th>\n",
       "      <th>noted_date</th>\n",
       "      <th>temp</th>\n",
       "      <th>out/in</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__export__.temp_log_196134_bd201015</td>\n",
       "      <td>Room Admin</td>\n",
       "      <td>08-12-2018 09:30</td>\n",
       "      <td>29</td>\n",
       "      <td>In</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__export__.temp_log_196131_7bca51bc</td>\n",
       "      <td>Room Admin</td>\n",
       "      <td>08-12-2018 09:30</td>\n",
       "      <td>29</td>\n",
       "      <td>In</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__export__.temp_log_196127_522915e3</td>\n",
       "      <td>Room Admin</td>\n",
       "      <td>08-12-2018 09:29</td>\n",
       "      <td>41</td>\n",
       "      <td>Out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__export__.temp_log_196128_be0919cf</td>\n",
       "      <td>Room Admin</td>\n",
       "      <td>08-12-2018 09:29</td>\n",
       "      <td>41</td>\n",
       "      <td>Out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__export__.temp_log_196126_d30b72fb</td>\n",
       "      <td>Room Admin</td>\n",
       "      <td>08-12-2018 09:29</td>\n",
       "      <td>31</td>\n",
       "      <td>In</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97601</th>\n",
       "      <td>__export__.temp_log_91076_7fbd08ca</td>\n",
       "      <td>Room Admin</td>\n",
       "      <td>28-07-2018 07:07</td>\n",
       "      <td>31</td>\n",
       "      <td>In</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97602</th>\n",
       "      <td>__export__.temp_log_147733_62c03f31</td>\n",
       "      <td>Room Admin</td>\n",
       "      <td>28-07-2018 07:07</td>\n",
       "      <td>31</td>\n",
       "      <td>In</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97603</th>\n",
       "      <td>__export__.temp_log_100386_84093a68</td>\n",
       "      <td>Room Admin</td>\n",
       "      <td>28-07-2018 07:06</td>\n",
       "      <td>31</td>\n",
       "      <td>In</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97604</th>\n",
       "      <td>__export__.temp_log_123297_4d8e690b</td>\n",
       "      <td>Room Admin</td>\n",
       "      <td>28-07-2018 07:06</td>\n",
       "      <td>31</td>\n",
       "      <td>In</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97605</th>\n",
       "      <td>__export__.temp_log_133741_32958703</td>\n",
       "      <td>Room Admin</td>\n",
       "      <td>28-07-2018 07:06</td>\n",
       "      <td>31</td>\n",
       "      <td>In</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97606 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        id  room_id/id        noted_date  \\\n",
       "0      __export__.temp_log_196134_bd201015  Room Admin  08-12-2018 09:30   \n",
       "1      __export__.temp_log_196131_7bca51bc  Room Admin  08-12-2018 09:30   \n",
       "2      __export__.temp_log_196127_522915e3  Room Admin  08-12-2018 09:29   \n",
       "3      __export__.temp_log_196128_be0919cf  Room Admin  08-12-2018 09:29   \n",
       "4      __export__.temp_log_196126_d30b72fb  Room Admin  08-12-2018 09:29   \n",
       "...                                    ...         ...               ...   \n",
       "97601   __export__.temp_log_91076_7fbd08ca  Room Admin  28-07-2018 07:07   \n",
       "97602  __export__.temp_log_147733_62c03f31  Room Admin  28-07-2018 07:07   \n",
       "97603  __export__.temp_log_100386_84093a68  Room Admin  28-07-2018 07:06   \n",
       "97604  __export__.temp_log_123297_4d8e690b  Room Admin  28-07-2018 07:06   \n",
       "97605  __export__.temp_log_133741_32958703  Room Admin  28-07-2018 07:06   \n",
       "\n",
       "       temp out/in  \n",
       "0        29     In  \n",
       "1        29     In  \n",
       "2        41    Out  \n",
       "3        41    Out  \n",
       "4        31     In  \n",
       "...     ...    ...  \n",
       "97601    31     In  \n",
       "97602    31     In  \n",
       "97603    31     In  \n",
       "97604    31     In  \n",
       "97605    31     In  \n",
       "\n",
       "[97606 rows x 5 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"static\\\\datasets\\\\iot_temp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static\\datasets\\all_sites_combined_concentration_data_clean_version.xlsx\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 2 fields in line 5, saw 3\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m df_path \u001b[38;5;129;01min\u001b[39;00m df_paths:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(df_path)\n\u001b[1;32m----> 5\u001b[0m     dfs\u001b[38;5;241m.\u001b[39mappend(\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\MANDR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\MANDR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\MANDR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1741\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m     (\n\u001b[0;32m   1745\u001b[0m         index,\n\u001b[0;32m   1746\u001b[0m         columns,\n\u001b[0;32m   1747\u001b[0m         col_dict,\n\u001b[1;32m-> 1748\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1749\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1751\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\MANDR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:843\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:904\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2058\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 2 fields in line 5, saw 3\n"
     ]
    }
   ],
   "source": [
    "# dfs = [pd.read_csv(df_path) for df_path in df_paths]\n",
    "dfs = []\n",
    "for df_path in df_paths:\n",
    "    print(df_path)\n",
    "    dfs.append(pd.read_csv(df_path, encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
