{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "from config import *\n",
    "from sys import getsizeof\n",
    "\n",
    "import os, sys, re, time, json, pickle, datetime, importlib\n",
    "import pandas as pd\n",
    "import dateutil.parser as dp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! Functions to load datasets (uncomment to run)\n",
    "\n",
    "def load_datasets(dfs: dict[str: pd.DataFrame]) -> None:\n",
    "    for name, path in DATASETS_ABS_PATHS.items():\n",
    "        print(f\"{name} {path}\")\n",
    "        if path.endswith(\".xlsx\"):\n",
    "            dfs[name] = pd.read_excel(path)\n",
    "        elif path.endswith(\".csv\"):\n",
    "            dfs[name] = pd.read_csv(path, low_memory=False)\n",
    "\n",
    "\n",
    "def normilize_datasets_timestamps(dfs: dict[str: pd.DataFrame]) -> None:\n",
    "    dfs[\"BWQAS\"][\"measurement_timestamp\"] = pd.to_datetime(dfs[\"BWQAS\"][\"measurement_timestamp\"], format=\"%m/%d/%Y %I:%M:%S %p\")\n",
    "    dfs[\"BWSAS\"][\"measurement_timestamp\"] = pd.to_datetime(dfs[\"BWSAS\"][\"measurement_timestamp\"], format=\"%m/%d/%Y %I:%M:%S %p\")\n",
    "    dfs[\"IOTTEMP\"][\"noted_date\"] = pd.to_datetime(dfs[\"IOTTEMP\"][\"noted_date\"], format=\"%d-%m-%Y %H:%M\")\n",
    "    dfs[\"IOT1\"][\"created_at\"] = pd.to_datetime(dfs[\"IOT1\"][\"created_at\"], format=\"%Y-%m-%d %H:%M:%S %Z\").dt.tz_convert(None)\n",
    "    dfs[\"IOT3\"][\"created_at\"] = pd.to_datetime(dfs[\"IOT3\"][\"created_at\"], format=\"%Y-%m-%d %H:%M:%S %Z\").dt.tz_convert(None)\n",
    "    dfs[\"IOT8\"][\"created_at\"] = pd.to_datetime(dfs[\"IOT8\"][\"created_at\"], format=\"%Y-%m-%dT%H:%M:%S%z\").dt.tz_convert(None)\n",
    "    dfs[\"IOT9\"][\"created_at\"] = pd.to_datetime(dfs[\"IOT9\"][\"created_at\"], format=\"%Y-%m-%d %H:%M:%S %Z\").dt.tz_convert(None)\n",
    "    # dfs[\"IOT2\"][\"created_at\"] = pd.to_datetime(dfs[\"IOT2\"][\"created_at\"].map({'%Y-%m-%d': '%Y-%m-%d', '%d/%m/%Y': '%d/%m/%Y', '%b %d, %Y': '%b %d, %Y'}), format=\"%Y-%m-%d %H:%M:%S %Z\")\n",
    "    # dfs[\"IOT4\"][\"created_at\"] = pd.to_datetime(dfs[\"IOT4\"][\"created_at\"], format=\"%Y-%m-%d %H:%M:%S %Z\")\n",
    "    # dfs[\"IOT6\"][\"created_at\"] = pd.to_datetime(dfs[\"IOT6\"][\"created_at\"], format=\"%Y-%m-%d %H:%M:%S %Z\")\n",
    "    # dfs[\"IOT7\"][\"created_at\"] = pd.to_datetime(dfs[\"IOT7\"][\"created_at\"], format=\"%Y-%m-%dT%H:%M:%S%z\")\n",
    "    # dfs[\"IOT10\"][\"created_at\"] = pd.to_datetime(dfs[\"IOT10\"][\"created_at\"], format=\"%Y-%m-%d %H:%M:%S %Z\")\n",
    "    # dfs[\"IOT11\"][\"created_at\"] = pd.to_datetime(dfs[\"IOT11\"][\"created_at\"], format=\"%Y-%m-%d %H:%M:%S %Z\")\n",
    "    # dfs[\"IOT12\"][\"created_at\"] = pd.to_datetime(dfs[\"IOT12\"][\"created_at\"], format=\"%Y-%m-%d %H:%M:%S %Z\")\n",
    "\n",
    "\n",
    "def normilize_datasets_columns_names(dfs: dict[str: pd.DataFrame]) -> None:\n",
    "    for df_name in dfs:\n",
    "        dfs[df_name].columns = dfs[df_name].columns.str.strip().str.rstrip('-').str.lower()\n",
    "        dfs[df_name].columns = [re.sub(r'[^\\w\\s]', '', col.replace(' ', '_').replace('-', '_')) for col in dfs[df_name].columns]\n",
    "\n",
    "\n",
    "def normilize_datasets_dtypes(dfs: dict[str: pd.DataFrame]) -> None:\n",
    "    for df_name in dfs:\n",
    "        dfs[df_name] = dfs[df_name].convert_dtypes()\n",
    "\n",
    "\n",
    "def normilize_datasets(dfs: dict[str: pd.DataFrame]) -> None:\n",
    "    normilize_datasets_columns_names(dfs)\n",
    "    normilize_datasets_dtypes\n",
    "    normilize_datasets_timestamps(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! Import pickled datasets (and save them if you want) \n",
    "\n",
    "# with open(\".\\\\static\\\\pickles\\\\dfs.pickle\", \"wb\") as f:\n",
    "#     pickle.dump(dfs, f)\n",
    "with open(\".\\\\static\\\\pickles\\\\dfs.pickle\", \"rb\") as f:\n",
    "    dfs: list[pd.DataFrame] = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to automatically convert datetime dtype in datasets (works poorly) \n",
    "\n",
    "def try_to_datetime(df: pd.DataFrame):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == \"string[python]\" and any(char in df[col].iloc[0] for char in ['-', '/', ':']):\n",
    "            try:\n",
    "                df[col] = pd.to_datetime(df[col], format=\"mixed\")\n",
    "            except:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of loading single dataset (outdated)\n",
    "\n",
    "# df = pd.read_csv(DATASETS_ABS_PATHS[\"BWSAS\"])\n",
    "# df.rename(columns=lambda x: x.replace(' ', '_'), inplace=True) # Replace spaces with underscores in column names\n",
    "# df.Measurement_Timestamp = pd.to_datetime(df.Measurement_Timestamp, format=\"%m/%d/%Y %I:%M:%S %p\")\n",
    "# df = df.convert_dtypes()\n",
    "# df_dict = df.to_dict(orient=\"records\")\n",
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# proto files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating proto template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! Function to create ptorobuf template\n",
    "\n",
    "def create_proto_template(package_name: str, df: pd.DataFrame):\n",
    "    d = []\n",
    "    timestamp_flag = False\n",
    "    \n",
    "    for name, dtype in df.dtypes.to_dict().items():\n",
    "        match dtype:\n",
    "            case \"string\":\n",
    "                d.append([\"string\", name])\n",
    "            case \"Float64\":\n",
    "                d.append([\"float\", name])\n",
    "            case \"Int64\":\n",
    "                d.append([\"int64\", name])\n",
    "            case \"datetime64[ns]\":\n",
    "                timestamp_flag = True\n",
    "                d.append([\"google.protobuf.Timestamp\", name])\n",
    "    \n",
    "    template = f'\\\n",
    "syntax = \"proto3\";\\n\\\n",
    "{'import \"google/protobuf/timestamp.proto\";\\n' if timestamp_flag else ''}\\\n",
    "\\npackage {package_name};\\n\\\n",
    "message {package_name}_message {{\\n\\\n",
    "'\n",
    "\n",
    "    template += \"\\n\".join(f\"    {e[0]} {e[1]} = {d.index(e) + 1};\" for e in d)\n",
    "    template += \"\\n}\"\n",
    "    return template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create ptorobuf template (Does NOT work (yoinked from net))\n",
    "\n",
    "import pandas as pd\n",
    "from google.protobuf.descriptor_pb2 import FieldDescriptorProto\n",
    "from google.protobuf.descriptor_pb2 import FileDescriptorProto\n",
    "from google.protobuf.compiler.plugin_pb2 import CodeGeneratorRequest\n",
    "from google.protobuf.compiler.plugin_pb2 import CodeGeneratorResponse\n",
    "\n",
    "def create_protobuf_template(df):\n",
    "    # Create a FileDescriptorProto\n",
    "    file_descriptor_proto = FileDescriptorProto()\n",
    "\n",
    "    # Create a message type for the DataFrame\n",
    "    message_type = file_descriptor_proto.message_type.add()\n",
    "    message_type.name = \"DataFrame\"\n",
    "\n",
    "    # Iterate over the columns of the DataFrame\n",
    "    for column_name, column_type in df.dtypes.items():\n",
    "        # Create a field descriptor for each column\n",
    "        field_descriptor = message_type.field.add()\n",
    "        field_descriptor.name = column_name\n",
    "\n",
    "        # Map Pandas dtype to Protobuf field type\n",
    "        if column_type == \"int64\":\n",
    "            field_descriptor.type = FieldDescriptorProto.TYPE_INT64\n",
    "        elif column_type == \"float64\":\n",
    "            field_descriptor.type = FieldDescriptorProto.TYPE_DOUBLE\n",
    "        elif column_type == \"bool\":\n",
    "            field_descriptor.type = FieldDescriptorProto.TYPE_BOOL\n",
    "        elif column_type == \"object\":\n",
    "            field_descriptor.type = FieldDescriptorProto.TYPE_STRING\n",
    "        elif column_type == \"datetime64[ns]\":\n",
    "            field_descriptor.type = FieldDescriptorProto.TYPE_STRING\n",
    "        else:\n",
    "            field_descriptor.type = FieldDescriptorProto.TYPE_STRING\n",
    "\n",
    "    # Return the serialized FileDescriptorProto\n",
    "    return file_descriptor_proto.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of creating proto template\n",
    "# package_name = \"m_beach\"\n",
    "\n",
    "# protobuf_template = create_proto_template(package_name, df)\n",
    "# print(protobuf_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compiling proto file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! Function to compile proto file from template file (creates {proto_filename}_pb2.py file)\n",
    "def compile_proto(proto_filename):\n",
    "    proto_abs_path = os.path.join(PROTOS_FOLDER, proto_filename)\n",
    "    compile_proto_command = f\"{PROTOC_EXE} --python_out=. {proto_abs_path} \"\n",
    "    os.system(compile_proto_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of compiling proto file\n",
    "\n",
    "# compile_proto(proto_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading data into python descriptor of proto message "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! Util functions for timestamps calculaitons, conversion and validation\n",
    "def str2unix(dt: str) -> tuple[int, int]:\n",
    "    # Converts ISO8601 timstamp string into POSIX timestamp tuple (seconds, nanoseconds)\n",
    "    dt_unix = dp.parse(dt).timestamp()\n",
    "    \n",
    "    seconds = int(dt_unix)\n",
    "    nanos   = int(dt_unix % 1 * 1e9)\n",
    "\n",
    "    return (seconds, nanos)\n",
    "\n",
    "\n",
    "def pdTimestamp2unix(dt: pd.Timestamp) -> tuple[int, int]:\n",
    "    return str2unix(dt.isoformat())\n",
    "\n",
    "\n",
    "def datetime_valid(dt_str):\n",
    "    try:\n",
    "        datetime.datetime.fromisoformat(dt_str)\n",
    "    except:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! json encoder and decoder for timestamp conversion handling\n",
    "class _JSONDecoder(json.JSONDecoder):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        json.JSONDecoder.__init__(\n",
    "            self, object_hook=self.object_hook, *args, **kwargs)\n",
    "\n",
    "    def object_hook(self, obj):\n",
    "        for key, value in obj.items():\n",
    "            if isinstance(value, str):\n",
    "                try:\n",
    "                    obj[key] = datetime.date.fromisoformat(value)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "        return obj\n",
    "\n",
    "    \n",
    "class _JSONEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (datetime.date, datetime.datetime, pd.Timestamp)):\n",
    "            return obj.isoformat()\n",
    "        return super().default(obj)\n",
    "\n",
    "\n",
    "def json_serialize(message: dict) -> bytes:\n",
    "    # Serializes dict to json: dict -> str -> bytes \n",
    "    return json.dumps(message, cls=_JSONEncoder, separators=(',', ':')).encode(ENCODING)\n",
    "\n",
    "\n",
    "def json_deserialize(message: bytes) -> dict:\n",
    "    # Serializes json to dict: bytes -> str -> dict \n",
    "    return json.loads(message.decode(ENCODING), cls=_JSONDecoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! Functions to fill package_message(s) with data from DataFrame(s)\n",
    "def fill_package_from_df(package_name: str, df_dict: list[dict], item: int):\n",
    "    # df_dict - df.to_dict(orient=\"records\"); item - number of record in df\n",
    "    package = importlib.import_module(PROTOS_FOLDER + \".\" + package_name + \"_pb2\")\n",
    "    package_message = getattr(package, package_name + \"_message\")()\n",
    "\n",
    "    for field, value in df_dict[item].items():\n",
    "        if value is None:\n",
    "            continue\n",
    "        if type(value) is pd.Timestamp:\n",
    "            seconds, nanos = pdTimestamp2unix(value)\n",
    "            setattr(getattr(package_message, field), \"seconds\", seconds)\n",
    "            setattr(getattr(package_message, field), \"nanos\", nanos)\n",
    "        else:\n",
    "            try:\n",
    "                setattr(package_message, field, value)\n",
    "            except Exception as e:\n",
    "                print(f\"Exception: {e} | field: {field} | value: {value}\")\n",
    "    \n",
    "    return package_message\n",
    "\n",
    "\n",
    "def fill_packages_from_df(package_name: str, df_dict: list[dict]) -> list:\n",
    "    package = importlib.import_module(PROTOS_FOLDER + \".\" + package_name + \"_pb2\")\n",
    "    package_messages = []\n",
    "    \n",
    "    for item in range(len(df_dict)):\n",
    "        package_message = None\n",
    "        package_message = getattr(package, package_name + \"_message\")()\n",
    "        for field, value in df_dict[item].items():\n",
    "            \n",
    "            if value is None:\n",
    "                continue\n",
    "            if type(value) is pd.Timestamp:\n",
    "                seconds, nanos = pdTimestamp2unix(value)\n",
    "                setattr(getattr(package_message, field), \"seconds\", seconds)\n",
    "                setattr(getattr(package_message, field), \"nanos\", nanos)\n",
    "            else:\n",
    "                try:\n",
    "                    setattr(package_message, field, value)\n",
    "                except Exception as e:\n",
    "                    print(f\"Exception: {e} | field: {field} | value: {value}\")\n",
    "        \n",
    "        package_messages.append(package_message)\n",
    "    \n",
    "    return package_messages\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! Function to fill package_message with data from dict\n",
    "def fill_package_from_dict(package_name: str, message: dict):\n",
    "    # message - dict (json)\n",
    "    package = importlib.import_module(PROTOS_FOLDER + \".\" + package_name + \"_pb2\")\n",
    "    package_message = getattr(package, package_name + \"_message\")()\n",
    "\n",
    "    for field, value in message.items():\n",
    "        if value is None:\n",
    "            continue\n",
    "        if datetime_valid(value):\n",
    "            seconds, nanos = str2unix(value)\n",
    "            setattr(getattr(package_message, field), \"seconds\", seconds)\n",
    "            setattr(getattr(package_message, field), \"nanos\", nanos)\n",
    "        elif type(value) is pd.Timestamp:\n",
    "            seconds, nanos = pdTimestamp2unix(value)\n",
    "            setattr(getattr(package_message, field), \"seconds\", seconds)\n",
    "            setattr(getattr(package_message, field), \"nanos\", nanos)\n",
    "        else:\n",
    "            try:\n",
    "                setattr(package_message, field, value)\n",
    "            except Exception as e:\n",
    "                print(f\"Exception: {e} | field: {field} | value: {value}\")\n",
    "    \n",
    "    return package_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of filling message with data from df and dicts with pd.Timestamp and timestamp in str \n",
    "# \n",
    "# NOTE: json format does not specify timestamp format, so i used most commonly used - ISO8601, \n",
    "# using custom JSONEncoder/JSONDecoder\n",
    "\n",
    "# item = 0\n",
    "# message = dfs_dict[\"BWQAS\"][item] # dict with pd.Timestamp as timestamp\n",
    "# message_json = json.dumps(message, cls=_JSONEncoder)\n",
    "\n",
    "# package_message = fill_package_from_df(package_name, df_dict, item)\n",
    "# package_message1 = fill_package_from_dict(package_name, message)\n",
    "# # message = json.loads(message_json) # dict with str ISO8601 as timestamp\n",
    "# package_message2 = fill_package_from_dict(package_name, json.loads(message_json))\n",
    "\n",
    "# print(f\"Same?: {package_message == package_message1 == package_message2}\")\n",
    "# print(# message, message_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## serializing/deserializing + io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util funcitons for bit <-> byte conversion and size finding\n",
    "\n",
    "def to_bits(byte_string: bytes) -> str:\n",
    "    # Convert byte string into bit string\n",
    "    return bin(int.from_bytes(byte_string, byteorder='big'))[2:]  # Remove '0b' prefix from binary string\n",
    "\n",
    "\n",
    "def to_bytes(bit_string: str) -> bytes:\n",
    "    # Convert bit string into byte string\n",
    "    return int(bit_string, 2).to_bytes((len(bit_string) + 7) // 8, byteorder='big')\n",
    "\n",
    "\n",
    "def bitsize(data: bytes | str) -> int:\n",
    "    if type(data) == bytes:\n",
    "        return len(to_bits(data))\n",
    "    elif type(data) == str and set(data) <= {'0', '1'}:\n",
    "        return len(data)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! Serialize/deserialize functions\n",
    "\n",
    "def proto_serialize(package_message):\n",
    "    return package_message.SerializeToString()\n",
    "\n",
    "\n",
    "def proto_serialize_l(package_messages: list) -> list:\n",
    "    return [package_message.SerializeToString() for package_message in package_messages]\n",
    "\n",
    "\n",
    "def proto_deserialize(package_message_serialized, package_name: str):\n",
    "    package = importlib.import_module(PROTOS_FOLDER + \".\" + package_name + \"_pb2\")\n",
    "    package_class = getattr(package, package_name + \"_message\")\n",
    "\n",
    "    package_message = package_class.FromString(package_message_serialized)\n",
    "    \n",
    "    return package_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! IO funcitons for package_message \n",
    "\n",
    "# writing package_message to the file\n",
    "def write_package(package_message, package_bin_filename):\n",
    "    with open(package_bin_filename, \"wb\") as f:\n",
    "        package_message_serialized = proto_serialize(package_message)\n",
    "        f.write(package_message_serialized)\n",
    "    \n",
    "    return package_message_serialized\n",
    "    \n",
    "\n",
    "\n",
    "# reading package_message from the file\n",
    "def read_package(package_bin_filename, package_name):\n",
    "    with open(package_bin_filename, \"rb\") as f:\n",
    "        package_message_serialized = f.read()\n",
    "        \n",
    "        package_message = proto_deserialize(package_message_serialized, package_name)\n",
    "\n",
    "    return package_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of serialization, dumping json, bit/byte conversion + size finding\n",
    "\n",
    "# package_bin_filename = package_name + \".bin\"\n",
    "\n",
    "# package_message_serialized   = write_package(package_message, package_bin_filename)\n",
    "# package_message_deserialized = read_package(package_bin_filename, package_name)\n",
    "\n",
    "# package_json_filename = package_name + \".json\"\n",
    "# with open(package_name + \".json\", \"w\") as f:\n",
    "#     json.dump(message, f, cls=_JSONEncoder)\n",
    "# with open(package_name + \"1.json\", \"w\") as f:\n",
    "#     f.write(message_json)\n",
    "\n",
    "# to_bits(package_message_serialized), bitsize(package_message_serialized), to_bytes(to_bits(package_message_serialized)) == package_message_serialized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of sizes\n",
    "\n",
    "# print(f\"message size (dict):                {getsizeof(message)}\")\n",
    "# print(f\"message_json size (str of json):    {getsizeof(message_json)}\")\n",
    "# print()\n",
    "# print(f\"bin file size:                      {os.path.getsize(package_bin_filename)}\")\n",
    "# print(f\"json file size:                     {os.path.getsize(package_json_filename)}\")\n",
    "# print()\n",
    "# print(f\"package_message size:               {getsizeof(package_message)}\")\n",
    "# print(f\"package_message_serialized bits:    {bitsize(package_message_serialized)}\")\n",
    "# print()\n",
    "# print(f\"ratio of serialized/json files:     {os.path.getsize(package_json_filename) / os.path.getsize(package_bin_filename)}\")\n",
    "# print(f\"package_message == package_message_deserialized: {package_message == package_message_deserialized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: package_message have a lot of metadata in their DESCRIPTOR (might be useful)\n",
    "# print([field.name for field in package_message.DESCRIPTOR.fields])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## libraries algorithms comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! Imports of compression libraries \n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import zlib, gzip, bz2, lzma, lz4.frame, zstd, brotli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of compression algorithms (on message serialized with protobuf)\n",
    "\n",
    "# pb_msg = package_message_serialized\n",
    "\n",
    "# pb_ratio_zlib     = bitsize(pb_msg) / bitsize(zlib.compress(pb_msg))\n",
    "# pb_ratio_gzip     = bitsize(pb_msg) / bitsize(gzip.compress(pb_msg))\n",
    "# pb_ratio_bz2      = bitsize(pb_msg) / bitsize(bz2.compress(pb_msg))\n",
    "# pb_ratio_lzma     = bitsize(pb_msg) / bitsize(lzma.compress(pb_msg))\n",
    "# pb_ratio_lz4      = bitsize(pb_msg) / bitsize(lz4.frame.compress(pb_msg))\n",
    "# pb_ratio_zstd     = bitsize(pb_msg) / bitsize(zstd.compress(pb_msg, 1))\n",
    "# pb_ratio_brotli   = bitsize(pb_msg) / bitsize(brotli.compress(pb_msg))\n",
    "\n",
    "\n",
    "\n",
    "# print(f'pb_ratio_zlib:        {pb_ratio_zlib}\\\n",
    "#       \\npb_ratio_gzip:        {pb_ratio_gzip}\\\n",
    "#       \\npb_ratio_bz2:         {pb_ratio_bz2}\\\n",
    "#       \\npb_ratio_lzma:        {pb_ratio_lzma}\\\n",
    "#       \\npb_ration_lz4:        {pb_ratio_lz4}\\\n",
    "#       \\npb_ration_zstd(1):    {pb_ratio_zstd}\\\n",
    "#       \\npb_ration_brotli(11): {pb_ratio_brotli}\\\n",
    "# '\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of compression algorithms (on message serialized with json)\n",
    "\n",
    "# j_msg = bytes(message_json, \"utf-8\")\n",
    "\n",
    "# j_ratio_zlib      = bitsize(j_msg)  / bitsize(zlib.compress(j_msg))\n",
    "# j_ratio_gzip      = bitsize(j_msg)  / bitsize(gzip.compress(j_msg))\n",
    "# j_ratio_bz2       = bitsize(j_msg)  / bitsize(bz2.compress(j_msg)) \n",
    "# j_ratio_lzma      = bitsize(j_msg)  / bitsize(lzma.compress(j_msg))\n",
    "# j_ratio_lz4       = bitsize(j_msg)  / bitsize(lz4.frame.compress(j_msg))\n",
    "# j_ratio_zstd      = bitsize(j_msg)  / bitsize(zstd.compress(j_msg, 1))\n",
    "# j_ratio_brotli    = bitsize(j_msg)  / bitsize(brotli.compress(j_msg))\n",
    "\n",
    "# print(f'j_ratio_zlib:         {j_ratio_zlib}\\\n",
    "#       \\nj_ratio_gzip:         {j_ratio_gzip}\\\n",
    "#       \\nj_ratio_bz2:          {j_ratio_bz2}\\\n",
    "#       \\nj_ratio_lzma:         {j_ratio_lzma}\\\n",
    "#       \\nj_ration_lz4:         {j_ratio_lz4}\\\n",
    "#       \\nj_ration_zstd(1):     {j_ratio_zstd}\\\n",
    "#       \\nj_ration_brotli(11):  {j_ratio_brotli}\\\n",
    "# '\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huffman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huffman encoding algorithm\n",
    "\n",
    "import heapq\n",
    "from collections import defaultdict\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, char=None, freq=0, left=None, right=None):\n",
    "        self.char = char\n",
    "        self.freq = freq\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        if self.freq == other.freq:\n",
    "            return self.char < other.char if self.char and other.char else False\n",
    "        return self.freq < other.freq\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.freq == other.freq and self.char == other.char\n",
    "\n",
    "def build_huffman_tree(freq_dict):\n",
    "    priority_queue = [Node(char, freq) for char, freq in freq_dict.items()]\n",
    "    heapq.heapify(priority_queue)\n",
    "\n",
    "    while len(priority_queue) > 1:\n",
    "        left = heapq.heappop(priority_queue)\n",
    "        right = heapq.heappop(priority_queue)\n",
    "        new_node = Node(freq=left.freq + right.freq, left=left, right=right)\n",
    "        heapq.heappush(priority_queue, new_node)\n",
    "\n",
    "    return priority_queue[0]\n",
    "\n",
    "\n",
    "def build_frequency_dict(data):\n",
    "    freq_dict = defaultdict(int)\n",
    "    for char in data:\n",
    "        freq_dict[char] += 1\n",
    "    return freq_dict\n",
    "\n",
    "def build_codewords(node, current_code=\"\", code_dict=None):\n",
    "    if code_dict is None:\n",
    "        code_dict = {}\n",
    "\n",
    "    if node.char is not None:\n",
    "        code_dict[node.char] = current_code\n",
    "        return code_dict\n",
    "\n",
    "    code_dict = build_codewords(node.left, current_code + \"0\", code_dict)\n",
    "    code_dict = build_codewords(node.right, current_code + \"1\", code_dict)\n",
    "\n",
    "    return code_dict\n",
    "\n",
    "def huffman_encode(data):\n",
    "    freq_dict = build_frequency_dict(data)\n",
    "    huffman_tree = build_huffman_tree(freq_dict)\n",
    "    codewords = build_codewords(huffman_tree)\n",
    "\n",
    "    encoded_data = \"\".join(codewords[char] for char in data)\n",
    "    return encoded_data, huffman_tree\n",
    "\n",
    "def huffman_decode(encoded_data, huffman_tree):\n",
    "    decoded_data = bytearray()\n",
    "    current_node = huffman_tree\n",
    "\n",
    "    for bit in encoded_data:\n",
    "        if bit == \"0\":\n",
    "            current_node = current_node.left\n",
    "        else:\n",
    "            current_node = current_node.right\n",
    "\n",
    "        if current_node.char is not None:\n",
    "            decoded_data.append(ord(current_node.char))\n",
    "            current_node = huffman_tree\n",
    "\n",
    "    return bytes(decoded_data)\n",
    "\n",
    "\n",
    "def serialize_tree(node):\n",
    "    if node.char is not None:\n",
    "        return '1' + bin(node.char)[2:].zfill(8)\n",
    "    else:\n",
    "        return '0' + serialize_tree(node.left) + serialize_tree(node.right)\n",
    "\n",
    "\n",
    "def deserialize_tree(data):\n",
    "    def helper(index):\n",
    "        if data[index] == '1':\n",
    "            char = chr(int(data[index + 1:index + 9], 2))\n",
    "            return Node(char=char), index + 9\n",
    "        else:\n",
    "            left, index = helper(index + 1)\n",
    "            right, index = helper(index)\n",
    "            return Node(left=left, right=right), index\n",
    "\n",
    "    root, _ = helper(0)\n",
    "    return root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of huffman usage\n",
    "\n",
    "# encoded_data, huffman_tree = huffman_encode(package_message_serialized)\n",
    "\n",
    "# # Serialize the Huffman tree\n",
    "# serialized_tree = serialize_tree(huffman_tree)\n",
    "\n",
    "# # Deserialize the Huffman tree\n",
    "# deserialized_tree = deserialize_tree(serialized_tree)\n",
    "\n",
    "# # The rest of the code remains the same...\n",
    "# # encoded_data = \"\".join(build_codewords(deserialized_tree)[char] for char in package_message_serialized)\n",
    "# decoded_data = huffman_decode(encoded_data, deserialized_tree)\n",
    "\n",
    "# print(f\"Original data:      {package_message_serialized}\")\n",
    "# print(f\"Decoded data:       {decoded_data}\")\n",
    "# print(f\"Decoded data == Original data: {package_message_serialized == decoded_data}\")\n",
    "# print(f\"Encoded data:       {encoded_data}\")\n",
    "# print(f\"Serialized tree:    {serialized_tree}\")\n",
    "# print()\n",
    "# print(f\"Encoded data bit length:    {bitsize(encoded_data)}\")\n",
    "# print(f\"Serialized tree bit length: {bitsize(serialized_tree)}\")\n",
    "# print(f\"Compression ratio:          {(bitsize(encoded_data) + bitsize(serialized_tree)) / bitsize(package_message_serialized)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rANS PyComP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# counter = Counter(pb_msg)\n",
    "\n",
    "# pb_msg_chr = ''.join([chr(i) for i in counter.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from libs.PyComP import ANS\n",
    "# msg_counter = Counter(pb_msg_chr)\n",
    "# ans = ANS.rANS(list(msg_counter.keys()), list(msg_counter.values()))\n",
    "# msg_enc, final_state = ans.encode([chr(i) for i in pb_msg], 0)\n",
    "# msg_dec = ''.join(ans.decode(msg_enc, final_state))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pb_msg_chr, pb_msg_chr, msg_enc[2:], len(msg_enc[2:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ! simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages dict for all info in one place\n",
    "\n",
    "# \"NOTE: empty pd.Timestamp are pd.NaT, \n",
    "# that upon trying to fill into proto message \n",
    "# cannot be handled and dropped with\n",
    "# warning which is ok, dont worry\n",
    "\n",
    "def get_packages(dfs: list[pd.DataFrame]) -> dict:\n",
    "    \n",
    "    print(f\"Creating packages object:\")\n",
    "    packages = {df: {\"package_name\": df+\"_package\"} for df in dfs}\n",
    "\n",
    "    # Creating proto templates\n",
    "    print(f\" - creating protobuf templates\") \n",
    "    for package in packages:\n",
    "        packages[package][\"protobuf_template\"] = create_proto_template(packages[package][\"package_name\"], dfs[package])\n",
    "\n",
    "    # Compiling proto templates\n",
    "    print(f\" - compiling protobuf templates\") \n",
    "    for package in packages:\n",
    "        packages[df][\"proto_filename\"] = packages[df][\"package_name\"] + \".proto\"\n",
    "        with open(os.path.join(PROTOS_FOLDER_ABS_PATH, packages[package][\"proto_filename\"]), \"w\") as f:\n",
    "            f.write(packages[package][\"protobuf_template\"])\n",
    "        compile_proto(packages[package][\"proto_filename\"])\n",
    "\n",
    "\n",
    "    # Converting datasets from pd.DataFrame to dict for easier messages filling (~40 secs)\n",
    "    print(f\" - copying datasets to dict in packages\") \n",
    "    for package in packages:\n",
    "        packages[package][\"messages\"] = dfs[package].to_dict(orient=\"records\")\n",
    "    \n",
    "    # Serializing messages with protobuf (filling and serializing messages) (might take a while (~2 min))\n",
    "    print(f\" - serializing messages with protobuf (might take  while)\") \n",
    "    for package in packages:\n",
    "        packages[package][\"package_messages_proto\"] = proto_serialize_l(fill_packages_from_df(packages[package][\"package_name\"], packages[package][\"messages\"]))\n",
    "    \n",
    "    # Serializing messages with json    \n",
    "    print(f\" - serializing messages with json\") \n",
    "    for package in packages:\n",
    "        packages[package][\"package_messages_json\"] = [json_serialize(i) for i in packages[package][\"messages\"]]\n",
    "    \n",
    "    print(f\"All done!\") \n",
    "    return packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs = {}\n",
    "# load_datasets(dfs)\n",
    "# normilize_datasets(dfs)\n",
    "# packages = get_packages(dfs)\n",
    "\n",
    "# with open(\"static\\\\pickles\\\\packages.pickle\", 'wb') as f:\n",
    "#     pickle.dump(packages, f)\n",
    "\n",
    "with open(\"static\\\\pickles\\\\packages.pickle\", 'rb') as f:\n",
    "    packages = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message:         272   10    0\n",
      "proto_message:   130   97  772 b'\\n\\x0eMontrose Beach\\x12\\x06\\x08\\xd0\\xcd\\x80\\x91\\x05\\x1dff\\xa2A%=\\n\\x97?-\\x93\\x18d?5\\n\\xd7\\xa3=8\\x03Eff\\x16AJ\\x118/30/2013 8:00 AMR\\x19MontroseBeach201308300800'\n",
      "message_json:    334  293 2343 {\"beach_name\":\"Montrose Beach\",\"measurement_timestamp\":\"2013-08-30T08:00:00\",\"water_temperature\":20.3,\"turbidity\":1.18,\"transducer_depth\":0.891,\"wave_height\":0.08,\"wave_period\":3,\"battery_life\":9.4,\"measurement_timestamp_label\":\"8/30/2013 8:00 AM\",\"measurement_id\":\"MontroseBeach201308300800\"}\n",
      "json file size:  293\n"
     ]
    }
   ],
   "source": [
    "item = 0\n",
    "message = packages[\"BWQAS\"][\"messages\"][item] # dict with pd.Timestamp as timestamp\n",
    "message_json = json.dumps(message, cls=_JSONEncoder,separators=(',', ':'))\n",
    "proto_message = packages[\"BWQAS\"][\"package_messages_proto\"][0]\n",
    "with open('testWB.json', 'w') as f:\n",
    "    json.dump(message, f, cls = _JSONEncoder, separators=(',', ':'))\n",
    "\n",
    "print(f\"message:        {getsizeof(message):4} {len(message):4} {bitsize(message):4}\")\n",
    "print(f\"proto_message:  {getsizeof(proto_message):4} {len(proto_message):4} {bitsize(proto_message):4} {proto_message}\")\n",
    "print(f\"message_json:   {getsizeof(message_json):4} {len(message_json):4} {bitsize(message_json.encode(ENCODING)):4} {message_json}\")\n",
    "print(f\"json file size: {os.path.getsize('testWB.json'):4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mass compression functions\n",
    "COMPRESSORS = {\n",
    "    \"zlib\": zlib.compress,\n",
    "    \"gzip\": gzip.compress,\n",
    "    \"bz2\": bz2.compress,\n",
    "    # \"lzma\": lzma.compress,\n",
    "    \"lz4\": lz4.frame.compress,\n",
    "    \"zstd\": zstd.compress,\n",
    "    # \"brotli\": brotli.compress,\n",
    "}\n",
    "\n",
    "DECOMPRESSORS = {\n",
    "    \"zlib\": zlib.decompress,\n",
    "    \"gzip\": gzip.decompress,\n",
    "    \"bz2\": bz2.decompress,\n",
    "    # \"lzma\": lzma.decompress,\n",
    "    \"lz4\": lz4.frame.decompress,\n",
    "    \"zstd\": zstd.decompress,\n",
    "    # \"brotli\": brotli.decompress,\n",
    "}\n",
    "\n",
    "\n",
    "def compress_messages(messages: list) -> list:\n",
    "    result = {}\n",
    "    for compressor in COMPRESSORS:\n",
    "        temp = []\n",
    "        print(f\"  {compressor}\", end=\"\")\n",
    "        st = time.time()\n",
    "        for message in messages:\n",
    "            temp.append(COMPRESSORS[compressor](message))\n",
    "        print(f\" {time.time() - st:.3f}\")\n",
    "        result[compressor] = temp\n",
    "        \n",
    "    return result\n",
    "    # return {compressor: [COMPRESSORS[compressor](message) for message in messages] for compressor in COMPRESSORS}     \n",
    "\n",
    "def decompress_messages(messages: list) -> list:\n",
    "    return {decompressor: [DECOMPRESSORS[decompressor](message) for message in messages] for decompressor in DECOMPRESSORS}     \n",
    "\n",
    "\n",
    "def compress_packages(packages: dict) -> None:\n",
    "    print(f\"Compressing messages:\")\n",
    "    for package in packages:\n",
    "        print(f\"{package}\")\n",
    "        print(f\" json: \")\n",
    "        packages[package][\"package_messages_json_compressed\"] = compress_messages(packages[package][\"package_messages_json\"])\n",
    "        print(f\" json done!\\n protobuf: \")\n",
    "        packages[package][\"package_messages_proto_compressed\"] = compress_messages(packages[package][\"package_messages_proto\"])\n",
    "        print(f\" protobuf done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compress_packages(packages) # займет оч много времени и памяти (NOTE: снимать показания во время сжатия, чтобы не занимать хотя бы память)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset   proto   json\n",
      "----------------------\n",
      "ASCCDCV  294938 294938\n",
      "ARGAZAL   55139  55139\n",
      "ARMAZAL   27074  27074\n",
      "BWQAS     34923  34923\n",
      "BWSAS     59144  59144\n",
      "IOTNL    477426 477426\n",
      "IOTTEMP   97606  97606\n",
      "IOT1      83126  83126\n",
      "IOT2     172249 172249\n",
      "IOT3     169185 169185\n",
      "IOT4      89844  89844\n",
      "IOT6      91050  91050\n",
      "IOT7     279612 279612\n",
      "IOT8      70744  70744\n",
      "IOT9     151785 151785\n",
      "IOT10       620    620\n",
      "IOT11      3165   3165\n",
      "IOT12      3590   3590\n",
      "TAZAW     55139  55139\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset   proto   json\")\n",
    "print(f\"{'-'*22}\")\n",
    "for package in packages:\n",
    "    print(f\"{package:8} {len(packages[package]['package_messages_proto']):6} {len(packages[package][\"package_messages_json\"]):6}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASCCDCV\n",
      " json\n",
      " json_compressed\n",
      " proto\n",
      " proto_compressed\n",
      "ARGAZAL\n",
      " json\n",
      " json_compressed\n",
      " proto\n",
      " proto_compressed\n",
      "ARMAZAL\n",
      " json\n",
      " json_compressed\n",
      " proto\n",
      " proto_compressed\n",
      "BWQAS\n",
      " json\n",
      " json_compressed\n",
      " proto\n",
      " proto_compressed\n",
      "BWSAS\n",
      " json\n",
      " json_compressed\n",
      " proto\n",
      " proto_compressed\n",
      "IOTNL\n",
      " json\n",
      " json_compressed\n",
      " proto\n",
      " proto_compressed\n",
      "IOTTEMP\n",
      " json\n",
      " json_compressed\n",
      " proto\n",
      " proto_compressed\n",
      "IOT1\n",
      " json\n",
      " json_compressed\n",
      " proto\n",
      " proto_compressed\n",
      "IOT2\n",
      " json\n",
      " json_compressed\n",
      " proto\n",
      " proto_compressed\n",
      "IOT3\n",
      " json\n",
      " json_compressed\n",
      " proto\n",
      " proto_compressed\n",
      "IOT4\n",
      " json\n",
      " json_compressed\n",
      " proto\n",
      " proto_compressed\n",
      "IOT6\n",
      " json\n",
      " json_compressed\n",
      " proto\n",
      " proto_compressed\n",
      "IOT7\n",
      " json\n",
      " json_compressed\n",
      " proto\n",
      " proto_compressed\n",
      "IOT8\n",
      " json\n",
      " json_compressed\n",
      " proto\n",
      " proto_compressed\n",
      "IOT9\n",
      " json\n",
      " json_compressed\n",
      " proto\n",
      " proto_compressed\n",
      "IOT10\n",
      " json\n",
      " json_compressed\n",
      " proto\n",
      " proto_compressed\n",
      "IOT11\n",
      " json\n",
      " json_compressed\n",
      " proto\n",
      " proto_compressed\n",
      "IOT12\n",
      " json\n",
      " json_compressed\n",
      " proto\n",
      " proto_compressed\n",
      "TAZAW\n",
      " json\n",
      " json_compressed\n",
      " proto\n",
      " proto_compressed\n"
     ]
    }
   ],
   "source": [
    "# Mass evaluation funcitons\n",
    "\n",
    "def get_sizes(packages):\n",
    "    sizes = {}\n",
    "    for package in packages:\n",
    "        print(package)\n",
    "\n",
    "        print(\" json\")\n",
    "        \n",
    "        sizes[package] = {}\n",
    "        sizes[package][\"json\"] = [bitsize(message) for message in packages[package][\"package_messages_json\"]]\n",
    "        print(\" json_compressed\")\n",
    "        for compressor in COMPRESSORS:\n",
    "            sizes[package][\"json_compressed_\"+compressor] = [bitsize(message) for message in packages[package][\"package_messages_json_compressed\"][compressor]]\n",
    "        \n",
    "        print(\" proto\")\n",
    "        sizes[package][\"proto\"] = [bitsize(message) for message in packages[package][\"package_messages_proto\"]]\n",
    "        print(\" proto_compressed\")\n",
    "        for compressor in COMPRESSORS:\n",
    "            sizes[package][\"proto_compressed_\"+compressor] = [bitsize(message) for message in packages[package][\"package_messages_proto_compressed\"][compressor]]\n",
    "    \n",
    "    return sizes\n",
    "\n",
    "sizes = get_sizes(packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASCCDCV\n",
      " json average: 1617.6776881920946\n",
      " json_compressed_zlib average: 921.797496422977\n",
      " json_compressed_gzip average: 1015.797496422977\n",
      " json_compressed_bz2 average: 1117.5153761129457\n",
      " json_compressed_lz4 average: 1349.12223585974\n",
      " json_compressed_zstd average: 1018.661372898711\n",
      " proto average: 238.93964494232685\n",
      " proto_compressed_zlib average: 310.31489329960874\n",
      " proto_compressed_gzip average: 404.31489329960874\n",
      " proto_compressed_bz2 average: 688.2568607639572\n",
      " proto_compressed_lz4 average: 420.6741959327045\n",
      " proto_compressed_zstd average: 311.6777763462151\n",
      "ARGAZAL\n",
      " json average: 3918.1287473476123\n",
      " json_compressed_zlib average: 1481.7829304122308\n",
      " json_compressed_gzip average: 1575.7829304122308\n",
      " json_compressed_bz2 average: 1617.932951268612\n",
      " json_compressed_lz4 average: 2631.0215455485227\n",
      " json_compressed_zstd average: 1641.7880991675584\n",
      " proto average: 500.12577304630116\n",
      " proto_compressed_zlib average: 577.9914216797548\n",
      " proto_compressed_gzip average: 671.9914216797548\n",
      " proto_compressed_bz2 average: 1005.5225702315965\n",
      " proto_compressed_lz4 average: 682.0720179908957\n",
      " proto_compressed_zstd average: 573.0842053718784\n",
      "ARMAZAL\n",
      " json average: 5046.029918002511\n",
      " json_compressed_zlib average: 1527.722168870503\n",
      " json_compressed_gzip average: 1621.722168870503\n",
      " json_compressed_bz2 average: 1743.7239417891703\n",
      " json_compressed_lz4 average: 2770.2199157863633\n",
      " json_compressed_zstd average: 1608.3455713969122\n",
      " proto average: 640.263573908547\n",
      " proto_compressed_zlib average: 555.8128832089828\n",
      " proto_compressed_gzip average: 649.8128832089828\n",
      " proto_compressed_bz2 average: 979.0481642904632\n",
      " proto_compressed_lz4 average: 702.9878850557731\n",
      " proto_compressed_zstd average: 589.9781340031026\n",
      "BWQAS\n",
      " json average: 2372.1246456489994\n",
      " json_compressed_zlib average: 1642.0531168570856\n",
      " json_compressed_gzip average: 1736.0531168570856\n",
      " json_compressed_bz2 average: 1912.3526902041635\n",
      " json_compressed_lz4 average: 2204.1987515391\n",
      " json_compressed_zstd average: 1684.281476390917\n",
      " proto average: 760.9198522463706\n",
      " proto_compressed_zlib average: 746.8874094436331\n",
      " proto_compressed_gzip average: 840.8874094436331\n",
      " proto_compressed_bz2 average: 1148.8746384903932\n",
      " proto_compressed_lz4 average: 903.2000973570426\n",
      " proto_compressed_zstd average: 827.8986914068092\n",
      "BWSAS\n",
      " json average: 3913.167049912079\n",
      " json_compressed_zlib average: 2353.180305694576\n",
      " json_compressed_gzip average: 2447.180305694576\n",
      " json_compressed_bz2 average: 2587.251318815095\n",
      " json_compressed_lz4 average: 3355.8751521709723\n",
      " json_compressed_zstd average: 2524.444068713648\n",
      " proto average: 1072.1385094007844\n",
      " proto_compressed_zlib average: 1001.1909914784255\n",
      " proto_compressed_gzip average: 1095.1909914784255\n",
      " proto_compressed_bz2 average: 1447.708372785067\n",
      " proto_compressed_lz4 average: 1176.2421209251995\n",
      " proto_compressed_zstd average: 1074.69511700257\n",
      "IOTNL\n",
      " json average: 2000.6294881300976\n",
      " json_compressed_zlib average: 1323.7930527453468\n",
      " json_compressed_gzip average: 1417.7930527453468\n",
      " json_compressed_bz2 average: 1490.8027589615983\n",
      " json_compressed_lz4 average: 1892.4504279197195\n",
      " json_compressed_zstd average: 1417.0849597633978\n",
      " proto average: 462.77958887869534\n",
      " proto_compressed_zlib average: 526.9202724610725\n",
      " proto_compressed_gzip average: 620.9202724610725\n",
      " proto_compressed_bz2 average: 962.5207634272117\n",
      " proto_compressed_lz4 average: 637.7841634096175\n",
      " proto_compressed_zstd average: 533.8571841500044\n",
      "IOTTEMP\n",
      " json average: 1016.4804827572076\n",
      " json_compressed_zlib average: 951.1802348216298\n",
      " json_compressed_gzip average: 1045.1802348216297\n",
      " json_compressed_bz2 average: 1130.057353031576\n",
      " json_compressed_lz4 average: 1196.4418785730386\n",
      " json_compressed_zstd average: 955.313771694363\n",
      " proto average: 501.48048275720754\n",
      " proto_compressed_zlib average: 568.3466385263201\n",
      " proto_compressed_gzip average: 662.3466385263201\n",
      " proto_compressed_bz2 average: 946.6131385365654\n",
      " proto_compressed_lz4 average: 684.4804827572076\n",
      " proto_compressed_zstd average: 575.4804827572076\n",
      "IOT1\n",
      " json average: 1864.48892043404\n",
      " json_compressed_zlib average: 1471.6909992060246\n",
      " json_compressed_gzip average: 1565.6909992060246\n",
      " json_compressed_bz2 average: 1687.6495200057743\n",
      " json_compressed_lz4 average: 2016.2838341794384\n",
      " json_compressed_zstd average: 1526.2910280778576\n",
      " proto average: 386.1016769723071\n",
      " proto_compressed_zlib average: 463.86143926088107\n",
      " proto_compressed_gzip average: 557.8614392608811\n",
      " proto_compressed_bz2 average: 907.4718619926376\n",
      " proto_compressed_lz4 average: 569.1016769723071\n",
      " proto_compressed_zstd average: 460.1016769723071\n",
      "IOT2\n",
      " json average: 1924.8704317586748\n",
      " json_compressed_zlib average: 1541.9217470057881\n",
      " json_compressed_gzip average: 1635.9217470057881\n",
      " json_compressed_bz2 average: 1770.8318480803953\n",
      " json_compressed_lz4 average: 2079.3967047704195\n",
      " json_compressed_zstd average: 1586.3520717101405\n",
      " proto average: 522.993410701949\n",
      " proto_compressed_zlib average: 596.8871517396327\n",
      " proto_compressed_gzip average: 690.8871517396327\n",
      " proto_compressed_bz2 average: 1009.6536003111775\n",
      " proto_compressed_lz4 average: 705.993410701949\n",
      " proto_compressed_zstd average: 596.993410701949\n",
      "IOT3\n",
      " json average: 1856.6542364866862\n",
      " json_compressed_zlib average: 1464.399509412773\n",
      " json_compressed_gzip average: 1558.399509412773\n",
      " json_compressed_bz2 average: 1678.7507107604101\n",
      " json_compressed_lz4 average: 2009.7220143629754\n",
      " json_compressed_zstd average: 1520.3403966072642\n",
      " proto average: 385.1644767562136\n",
      " proto_compressed_zlib average: 462.11599728108285\n",
      " proto_compressed_gzip average: 556.1159972810829\n",
      " proto_compressed_bz2 average: 907.8229334751899\n",
      " proto_compressed_lz4 average: 568.1644767562136\n",
      " proto_compressed_zstd average: 459.1644767562136\n",
      "IOT4\n",
      " json average: 1896.8573527447577\n",
      " json_compressed_zlib average: 1522.7050888206224\n",
      " json_compressed_gzip average: 1616.7050888206224\n",
      " json_compressed_bz2 average: 1751.714839054361\n",
      " json_compressed_lz4 average: 2046.3067984506479\n",
      " json_compressed_zstd average: 1568.5866168024577\n",
      " proto average: 522.7512577356307\n",
      " proto_compressed_zlib average: 596.7397266372824\n",
      " proto_compressed_gzip average: 690.7397266372824\n",
      " proto_compressed_bz2 average: 1012.9893148123414\n",
      " proto_compressed_lz4 average: 705.7512577356307\n",
      " proto_compressed_zstd average: 596.7512577356307\n",
      "IOT6\n",
      " json average: 1891.292498627128\n",
      " json_compressed_zlib average: 1515.8795167490389\n",
      " json_compressed_gzip average: 1609.8795167490389\n",
      " json_compressed_bz2 average: 1747.393893465129\n",
      " json_compressed_lz4 average: 2040.9073476112026\n",
      " json_compressed_zstd average: 1564.963382756727\n",
      " proto average: 522.9069192751235\n",
      " proto_compressed_zlib average: 597.0267545304778\n",
      " proto_compressed_gzip average: 691.0267545304778\n",
      " proto_compressed_bz2 average: 1009.2763756177924\n",
      " proto_compressed_lz4 average: 705.9068863261944\n",
      " proto_compressed_zstd average: 596.9068863261944\n",
      "IOT7\n",
      " json average: 1999.963792684148\n",
      " json_compressed_zlib average: 1558.2817189534069\n",
      " json_compressed_gzip average: 1652.2817189534069\n",
      " json_compressed_bz2 average: 1780.1240433171681\n",
      " json_compressed_lz4 average: 2143.724031872738\n",
      " json_compressed_zstd average: 1621.6457948872007\n",
      " proto average: 579.3175114086663\n",
      " proto_compressed_zlib average: 629.8173898115961\n",
      " proto_compressed_gzip average: 723.8173898115961\n",
      " proto_compressed_bz2 average: 1023.1737836716593\n",
      " proto_compressed_lz4 average: 761.5137690800109\n",
      " proto_compressed_zstd average: 653.3160236327483\n",
      "IOT8\n",
      " json average: 1972.445210901278\n",
      " json_compressed_zlib average: 1516.9932149722945\n",
      " json_compressed_gzip average: 1610.9932149722945\n",
      " json_compressed_bz2 average: 1718.4439669795318\n",
      " json_compressed_lz4 average: 2132.370349428927\n",
      " json_compressed_zstd average: 1580.1101436164197\n",
      " proto average: 485.022051340043\n",
      " proto_compressed_zlib average: 544.512948094538\n",
      " proto_compressed_gzip average: 638.512948094538\n",
      " proto_compressed_bz2 average: 984.6634626258058\n",
      " proto_compressed_lz4 average: 664.7957706660635\n",
      " proto_compressed_zstd average: 559.0220513400429\n",
      "IOT9\n",
      " json average: 1962.6797839048654\n",
      " json_compressed_zlib average: 1490.5044833152156\n",
      " json_compressed_gzip average: 1584.5044833152156\n",
      " json_compressed_bz2 average: 1707.3514181243206\n",
      " json_compressed_lz4 average: 2100.4465197483282\n",
      " json_compressed_zstd average: 1574.129077313305\n",
      " proto average: 499.31207958625686\n",
      " proto_compressed_zlib average: 555.4676351418125\n",
      " proto_compressed_gzip average: 649.4676351418125\n",
      " proto_compressed_bz2 average: 996.82571400336\n",
      " proto_compressed_lz4 average: 678.47886813585\n",
      " proto_compressed_zstd average: 573.3120795862569\n",
      "IOT10\n",
      " json average: 1619.2451612903226\n",
      " json_compressed_zlib average: 1327.8\n",
      " json_compressed_gzip average: 1421.8\n",
      " json_compressed_bz2 average: 1488.316129032258\n",
      " json_compressed_lz4 average: 1798.574193548387\n",
      " json_compressed_zstd average: 1371.8709677419354\n",
      " proto average: 455.35483870967744\n",
      " proto_compressed_zlib average: 501.7096774193548\n",
      " proto_compressed_gzip average: 595.7096774193549\n",
      " proto_compressed_bz2 average: 905.4387096774194\n",
      " proto_compressed_lz4 average: 634.2774193548387\n",
      " proto_compressed_zstd average: 529.3548387096774\n",
      "IOT11\n",
      " json average: 1599.5838862559242\n",
      " json_compressed_zlib average: 1297.2698262243287\n",
      " json_compressed_gzip average: 1391.2698262243287\n",
      " json_compressed_bz2 average: 1449.7677725118483\n",
      " json_compressed_lz4 average: 1779.0834123222749\n",
      " json_compressed_zstd average: 1348.9535545023696\n",
      " proto average: 388.1440758293839\n",
      " proto_compressed_zlib average: 460.2979462875197\n",
      " proto_compressed_gzip average: 554.2979462875197\n",
      " proto_compressed_bz2 average: 872.5772511848342\n",
      " proto_compressed_lz4 average: 571.0707740916272\n",
      " proto_compressed_zstd average: 462.1440758293839\n",
      "IOT12\n",
      " json average: 1603.401114206128\n",
      " json_compressed_zlib average: 1295.1983286908078\n",
      " json_compressed_gzip average: 1389.1983286908078\n",
      " json_compressed_bz2 average: 1427.5883008356545\n",
      " json_compressed_lz4 average: 1783.349860724234\n",
      " json_compressed_zstd average: 1341.8284122562675\n",
      " proto average: 396.0066852367688\n",
      " proto_compressed_zlib average: 469.44456824512537\n",
      " proto_compressed_gzip average: 563.4445682451253\n",
      " proto_compressed_bz2 average: 882.5431754874652\n",
      " proto_compressed_lz4 average: 579.0066852367688\n",
      " proto_compressed_zstd average: 470.0066852367688\n",
      "TAZAW\n",
      " json average: 3950.469558751519\n",
      " json_compressed_zlib average: 1447.3818712707885\n",
      " json_compressed_gzip average: 1541.3818712707885\n",
      " json_compressed_bz2 average: 1636.2047371189176\n",
      " json_compressed_lz4 average: 2537.9900433450007\n",
      " json_compressed_zstd average: 1618.17330745933\n",
      " proto average: 466.8645060664865\n",
      " proto_compressed_zlib average: 545.8424164384555\n",
      " proto_compressed_gzip average: 639.8424164384555\n",
      " proto_compressed_bz2 average: 966.1756107292479\n",
      " proto_compressed_lz4 average: 647.4496998494714\n",
      " proto_compressed_zstd average: 538.4496998494714\n"
     ]
    }
   ],
   "source": [
    "# среднее по размеру для каждого датасета/формата/алгоритма сжатия\n",
    "for package in sizes:\n",
    "    print(f\"{package}\")\n",
    "    for msg_type in sizes[package]:\n",
    "        print(f\" {msg_type} average: {sum(sizes[package][msg_type])/len(sizes[package][msg_type])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
